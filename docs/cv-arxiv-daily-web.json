{"Talking Face": {"2401.04718": "|**2024-01-11**|**Jump Cut Smoothing for Talking Heads**|Xiaojuan Wang et.al.|[2401.04718](http://arxiv.org/abs/2401.04718)|null|\n", "2312.15197": "|**2023-12-23**|**TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**|Xize Cheng et.al.|[2312.15197](http://arxiv.org/abs/2312.15197)|null|\n", "2312.13578": "|**2023-12-21**|**DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation**|Chenxu Zhang et.al.|[2312.13578](http://arxiv.org/abs/2312.13578)|null|\n", "2312.10921": "|**2023-12-18**|**AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis**|Dongze Li et.al.|[2312.10921](http://arxiv.org/abs/2312.10921)|null|\n", "2312.11568": "|**2023-12-18**|**VectorTalker: SVG Talking Face Generation with Progressive Vectorisation**|Hao Hu et.al.|[2312.11568](http://arxiv.org/abs/2312.11568)|null|\n", "2312.10687": "|**2024-01-31**|**MM-TTS: Multi-modal Prompt based Style Transfer for Expressive Text-to-Speech Synthesis**|Wenhao Guan et.al.|[2312.10687](http://arxiv.org/abs/2312.10687)|null|\n", "2312.09767": "|**2023-12-15**|**DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models**|Yifeng Ma et.al.|[2312.09767](http://arxiv.org/abs/2312.09767)|null|\n", "2312.07669": "|**2023-12-12**|**GMTalker: Gaussian Mixture based Emotional talking video Portraits**|Yibo Xia et.al.|[2312.07669](http://arxiv.org/abs/2312.07669)|null|\n", "2312.07385": "|**2023-12-12**|**GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance**|Haiming Zhang et.al.|[2312.07385](http://arxiv.org/abs/2312.07385)|null|\n", "2312.06613": "|**2023-12-11**|**Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism**|Georgios Milis et.al.|[2312.06613](http://arxiv.org/abs/2312.06613)|**[link](https://github.com/g-milis/NEUTART)**|\n", "2312.06400": "|**2023-12-11**|**DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers**|Aaron Mir et.al.|[2312.06400](http://arxiv.org/abs/2312.06400)|null|\n", "2312.05572": "|**2023-12-09**|**R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning**|Zhiling Ye et.al.|[2312.05572](http://arxiv.org/abs/2312.05572)|null|\n", "2312.05430": "|**2023-12-09**|**FT2TF: First-Person Statement Text-To-Talking Face Generation**|Xingjian Diao et.al.|[2312.05430](http://arxiv.org/abs/2312.05430)|null|\n", "2312.03763": "|**2023-12-19**|**Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing**|Yushi Lan et.al.|[2312.03763](http://arxiv.org/abs/2312.03763)|null|\n", "2312.02781": "|**2023-12-05**|**PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features**|Tianshun Han et.al.|[2312.02781](http://arxiv.org/abs/2312.02781)|null|\n", "2312.02703": "|**2023-12-05**|**MyPortrait: Morphable Prior-Guided Personalized Portrait Generation**|Bo Ding et.al.|[2312.02703](http://arxiv.org/abs/2312.02703)|null|\n", "2312.01841": "|**2023-12-07**|**VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior**|Xusen Sun et.al.|[2312.01841](http://arxiv.org/abs/2312.01841)|null|\n", "2311.18729": "|**2023-11-30**|**Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data**|Yu Deng et.al.|[2311.18729](http://arxiv.org/abs/2311.18729)|null|\n", "2311.17590": "|**2023-11-29**|**SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis**|Ziqiao Peng et.al.|[2311.17590](http://arxiv.org/abs/2311.17590)|**[link](https://github.com/ZiqiaoPeng/SyncTalk)**|\n", "2311.17409": "|**2023-11-30**|**Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation**|Pramook Khungurn et.al.|[2311.17409](http://arxiv.org/abs/2311.17409)|null|\n", "2311.17177": "|**2023-11-28**|**THInImg: Cross-modal Steganography for Presenting Talking Heads in Images**|Lin Zhao et.al.|[2311.17177](http://arxiv.org/abs/2311.17177)|null|\n", "2311.15230": "|**2024-03-14**|**GAIA: Zero-shot Talking Avatar Generation**|Tianyu He et.al.|[2311.15230](http://arxiv.org/abs/2311.15230)|null|\n", "2311.14730": "|**2023-11-20**|**MemoryCompanion: A Smart Healthcare Solution to Empower Efficient Alzheimer's Care Via Unleashing Generative AI**|Lifei Zheng et.al.|[2311.14730](http://arxiv.org/abs/2311.14730)|null|\n", "2311.08673": "|**2023-11-15**|**CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding**|Jianzong Wang et.al.|[2311.08673](http://arxiv.org/abs/2311.08673)|null|\n", "2311.06772": "|**2023-11-12**|**ChatAnything: Facetime Chat with LLM-Enhanced Personas**|Yilin Zhao et.al.|[2311.06772](http://arxiv.org/abs/2311.06772)|null|\n", "2311.06307": "|**2023-11-08**|**Synthetic Speaking Children -- Why We Need Them and How to Make Them**|Muhammad Ali Farooq et.al.|[2311.06307](http://arxiv.org/abs/2311.06307)|null|\n", "2311.02549": "|**2023-11-05**|**3D-Aware Talking-Head Video Motion Transfer**|Haomiao Ni et.al.|[2311.02549](http://arxiv.org/abs/2311.02549)|null|\n", "2311.00994": "|**2023-11-02**|**LaughTalk: Expressive 3D Talking Head Generation with Laughter**|Kim Sung-Bin et.al.|[2311.00994](http://arxiv.org/abs/2311.00994)|null|\n", "2310.15112": "|**2023-10-23**|**The Self 2.0: How AI-Enhanced Self-Clones Transform Self-Perception and Improve Presentation Skills**|Qingxiao Zheng et.al.|[2310.15112](http://arxiv.org/abs/2310.15112)|null|\n", "2310.11295": "|**2023-10-17**|**CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation**|Zhaojie Chu et.al.|[2310.11295](http://arxiv.org/abs/2310.11295)|null|\n", "2310.05720": "|**2023-10-15**|**HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation**|Yaosen Chen et.al.|[2310.05720](http://arxiv.org/abs/2310.05720)|**[link](https://github.com/semchan/HyperLips)**|\n", "2310.05304": "|**2023-10-08**|**GestSync: Determining who is speaking without a talking head**|Sindhu B Hegde et.al.|[2310.05304](http://arxiv.org/abs/2310.05304)|**[link](https://github.com/Sindhu-Hegde/gestsync)**|\n", "2310.05058": "|**2023-11-03**|**Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading**|Songtao Luo et.al.|[2310.05058](http://arxiv.org/abs/2310.05058)|**[link](https://github.com/jinchiniao/LSHUC)**|\n", "2310.02739": "|**2023-12-13**|**uTalk: Bridging the Gap Between Humans and AI**|Hussam Azzuni et.al.|[2310.02739](http://arxiv.org/abs/2310.02739)|null|\n", "2310.00434": "|**2023-09-30**|**DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models**|Zhiyao Sun et.al.|[2310.00434](http://arxiv.org/abs/2310.00434)|null|\n", "2309.16148": "|**2023-09-28**|**OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous Head Motions**|Jin Liu et.al.|[2309.16148](http://arxiv.org/abs/2309.16148)|null|\n", "2309.07752": "|**2023-09-14**|**DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis**|Yaoyu Su et.al.|[2309.07752](http://arxiv.org/abs/2309.07752)|null|\n", "2309.07509": "|**2023-09-14**|**DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks**|Zipeng Qi et.al.|[2309.07509](http://arxiv.org/abs/2309.07509)|null|\n", "2309.07495": "|**2023-09-14**|**HDTR-Net: A Real-Time High-Definition Teeth Restoration Network for Arbitrary Talking Face Generation Methods**|Yongyuan Li et.al.|[2309.07495](http://arxiv.org/abs/2309.07495)|**[link](https://github.com/yylgoodlucky/hdtr)**|\n", "2309.06723": "|**2023-09-13**|**PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network**|Qinghua Liu et.al.|[2309.06723](http://arxiv.org/abs/2309.06723)|null|\n", "2309.04946": "|**2023-10-12**|**Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation**|Yuan Gan et.al.|[2309.04946](http://arxiv.org/abs/2309.04946)|**[link](https://github.com/yuangan/eat_code)**|\n", "2309.04814": "|**2023-09-09**|**Speech2Lip: High-fidelity Speech to Lip Generation by Learning from a Short Video**|Xiuzhe Wu et.al.|[2309.04814](http://arxiv.org/abs/2309.04814)|**[link](https://github.com/cvmi-lab/speech2lip)**|\n", "2309.01950": "|**2023-11-06**|**RADIO: Reference-Agnostic Dubbing Video Synthesis**|Dongyeun Lee et.al.|[2309.01950](http://arxiv.org/abs/2309.01950)|null|\n", "2308.16041": "|**2023-08-30**|**From Pixels to Portraits: A Comprehensive Survey of Talking Head Generation Techniques and Applications**|Shreyank N Gowda et.al.|[2308.16041](http://arxiv.org/abs/2308.16041)|null|\n", "2308.15224": "|**2023-08-29**|**Papeos: Augmenting Research Papers with Talk Videos**|Tae Soo Kim et.al.|[2308.15224](http://arxiv.org/abs/2308.15224)|null|\n", "2308.14256": "|**2023-12-14**|**FaceChain: A Playground for Human-centric Artificial Intelligence Generated Content**|Yang Liu et.al.|[2308.14256](http://arxiv.org/abs/2308.14256)|**[link](https://github.com/modelscope/facechain)**|\n", "2308.12866": "|**2023-08-24**|**ToonTalker: Cross-Domain Face Reenactment**|Yuan Gong et.al.|[2308.12866](http://arxiv.org/abs/2308.12866)|null|\n", "2308.09716": "|**2023-08-18**|**Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization**|Soumik Mukhopadhyay et.al.|[2308.09716](http://arxiv.org/abs/2308.09716)|**[link](https://github.com/soumik-kanad/diff2lip)**|\n", "2308.08849": "|**2023-08-17**|**A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation**|Li Liu et.al.|[2308.08849](http://arxiv.org/abs/2308.08849)|**[link](https://github.com/wentaol86/awesome-body-language)**|\n", "2308.06457": "|**2023-08-12**|**Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation**|Zhichao Wang et.al.|[2308.06457](http://arxiv.org/abs/2308.06457)|**[link](https://github.com/zhichaowang970201/text-to-video)**|\n", "2401.01207": "|**2024-04-07**|**Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation**|Renshuai Liu et.al.|[2401.01207](http://arxiv.org/abs/2401.01207)|null|\n", "2312.17205": "|**2024-04-11**|**EFHQ: Multi-purpose ExtremePose-Face-HQ dataset**|Trung Tuan Dao et.al.|[2312.17205](http://arxiv.org/abs/2312.17205)|null|\n", "2312.10422": "|**2023-12-19**|**Learning Dense Correspondence for NeRF-Based Face Reenactment**|Songlin Yang et.al.|[2312.10422](http://arxiv.org/abs/2312.10422)|null|\n", "2311.05521": "|**2023-11-28**|**BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis**|Hao-Bin Duan et.al.|[2311.05521](http://arxiv.org/abs/2311.05521)|**[link](https://github.com/buaavrcg/BakedAvatar)**|\n", "2309.05095": "|**2023-09-10**|**MaskRenderer: 3D-Infused Multi-Mask Realistic Face Reenactment**|Tina Behrouzi et.al.|[2309.05095](http://arxiv.org/abs/2309.05095)|null|\n", "2308.04830": "|**2023-08-11**|**VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer**|Liyang Chen et.al.|[2308.04830](http://arxiv.org/abs/2308.04830)|null|\n", "2308.00462": "|**2023-09-20**|**Context-Aware Talking-Head Video Editing**|Songlin Yang et.al.|[2308.00462](http://arxiv.org/abs/2308.00462)|null|\n", "2307.10797": "|**2023-07-20**|**HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces**|Stella Bounareli et.al.|[2307.10797](http://arxiv.org/abs/2307.10797)|**[link](https://github.com/stelabou/hyperreenact)**|\n", "2307.10008": "|**2023-07-19**|**MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions**|Yunfei Liu et.al.|[2307.10008](http://arxiv.org/abs/2307.10008)|null|\n", "2307.09906": "|**2023-08-18**|**Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation**|Fa-Ting Hong et.al.|[2307.09906](http://arxiv.org/abs/2307.09906)|**[link](https://github.com/harlanhong/iccv2023-mcnet)**|\n", "2307.09821": "|**2023-07-19**|**Hierarchical Semantic Perceptual Listener Head Video Generation: A High-performance Pipeline**|Zhigang Chang et.al.|[2307.09821](http://arxiv.org/abs/2307.09821)|null|\n", "2307.09480": "|**2023-07-18**|**FACTS: Facial Animation Creation using the Transfer of Styles**|Jack Saunders et.al.|[2307.09480](http://arxiv.org/abs/2307.09480)|null|\n", "2307.09368": "|**2023-12-11**|**Audio-driven Talking Face Generation by Overcoming Unintended Information Flow**|Dogucan Yaman et.al.|[2307.09368](http://arxiv.org/abs/2307.09368)|null|\n", "2307.09153": "|**2023-07-19**|**OPHAvatars: One-shot Photo-realistic Head Avatars**|Shaoxu Li et.al.|[2307.09153](http://arxiv.org/abs/2307.09153)|**[link](https://github.com/lsx0101/ophavatars)**|\n", "2307.05397": "|**2023-10-29**|**On the Vulnerability of DeepFake Detectors to Attacks Generated by Denoising Diffusion Models**|Marija Ivanovska et.al.|[2307.05397](http://arxiv.org/abs/2307.05397)|null|\n", "2307.04187": "|**2023-07-09**|**Predictive Coding For Animation-Based Video Compression**|Goluck Konuko et.al.|[2307.04187](http://arxiv.org/abs/2307.04187)|null|\n", "2307.03990": "|**2023-07-08**|**FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction**|Ganglai Wang et.al.|[2307.03990](http://arxiv.org/abs/2307.03990)|null|\n", "2307.02090": "|**2023-07-05**|**Interactive Conversational Head Generation**|Mohan Zhou et.al.|[2307.02090](http://arxiv.org/abs/2307.02090)|null|\n", "2307.03270": "|**2023-07-04**|**A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation**|Louis Airale et.al.|[2307.03270](http://arxiv.org/abs/2307.03270)|**[link](https://github.com/louisbearing/hmo-audio)**|\n", "2307.01233": "|**2023-07-03**|**RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations**|Neha Sahipjohn et.al.|[2307.01233](http://arxiv.org/abs/2307.01233)|null|\n", "2306.16003": "|**2024-01-18**|**Text-driven Talking Face Synthesis by Reprogramming Audio-driven Models**|Jeongsoo Choi et.al.|[2306.16003](http://arxiv.org/abs/2306.16003)|null|\n", "2306.11541": "|**2023-06-20**|**Audio-Driven 3D Facial Animation from In-the-Wild Videos**|Liying Lu et.al.|[2306.11541](http://arxiv.org/abs/2306.11541)|null|\n", "2306.10813": "|**2023-08-16**|**Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions**|Yuqi Sun et.al.|[2306.10813](http://arxiv.org/abs/2306.10813)|null|\n", "2306.10799": "|**2023-08-30**|**SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces**|Ziqiao Peng et.al.|[2306.10799](http://arxiv.org/abs/2306.10799)|**[link](https://github.com/psyai-net/SelfTalk_release)**|\n", "2306.08990": "|**2023-09-26**|**Emotional Speech-Driven Animation with Content-Emotion Disentanglement**|Radek Dan\u011b\u010dek et.al.|[2306.08990](http://arxiv.org/abs/2306.08990)|null|\n", "2306.07579": "|**2023-06-13**|**Parametric Implicit Face Representation for Audio-Driven Facial Reenactment**|Ricong Huang et.al.|[2306.07579](http://arxiv.org/abs/2306.07579)|null|\n", "2306.06885": "|**2023-06-12**|**NPVForensics: Jointing Non-critical Phonemes and Visemes for Deepfake Detection**|Yu Chen et.al.|[2306.06885](http://arxiv.org/abs/2306.06885)|null|\n", "2306.05356": "|**2023-06-08**|**ReliableSwap: Boosting General Face Swapping Via Reliable Supervision**|Ge Yuan et.al.|[2306.05356](http://arxiv.org/abs/2306.05356)|**[link](https://github.com/ygtxr1997/reliableswap)**|\n", "2306.03594": "|**2023-06-06**|**Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks**|Jianrong Wang et.al.|[2306.03594](http://arxiv.org/abs/2306.03594)|null|\n", "2306.03504": "|**2023-08-02**|**Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis**|Zhenhui Ye et.al.|[2306.03504](http://arxiv.org/abs/2306.03504)|null|\n", "2306.02903": "|**2023-06-05**|**Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions**|Shaoxu Li et.al.|[2306.02903](http://arxiv.org/abs/2306.02903)|**[link](https://github.com/lsx0101/instruct-video2avatar)**|\n", "2306.01415": "|**2023-07-26**|**Learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation**|Federico Nocentini et.al.|[2306.01415](http://arxiv.org/abs/2306.01415)|**[link](https://github.com/fedenoce/s2l-s2d)**|\n", "2305.19556": "|**2024-04-01**|**Exploring Phonetic Context-Aware Lip-Sync For Talking Face Generation**|Se Jin Park et.al.|[2305.19556](http://arxiv.org/abs/2305.19556)|null|\n", "2305.13962": "|**2023-05-23**|**CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation**|Jingning Xu et.al.|[2305.13962](http://arxiv.org/abs/2305.13962)|null|\n", "2305.13353": "|**2023-05-22**|**RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars**|Dongwei Pan et.al.|[2305.13353](http://arxiv.org/abs/2305.13353)|**[link](https://github.com/renderme-360/renderme-360)**|\n", "2305.10945": "|**2023-05-18**|**An Android Robot Head as Embodied Conversational Agent**|Marcel Heisler et.al.|[2305.10945](http://arxiv.org/abs/2305.10945)|null|\n", "2305.10456": "|**2023-05-17**|**LPMM: Intuitive Pose Control for Neural Talking-Head Model via Landmark-Parameter Morphable Model**|Kwangho Lee et.al.|[2305.10456](http://arxiv.org/abs/2305.10456)|null|\n", "2305.08293": "|**2023-05-15**|**Identity-Preserving Talking Face Generation with Landmark and Appearance Priors**|Weizhi Zhong et.al.|[2305.08293](http://arxiv.org/abs/2305.08293)|**[link](https://github.com/Weizhi-Zhong/IP_LAP)**|\n", "2305.06225": "|**2023-12-10**|**DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation**|Fa-Ting Hong et.al.|[2305.06225](http://arxiv.org/abs/2305.06225)|**[link](https://github.com/harlanhong/cvpr2022-dagan)**|\n", "2305.05445": "|**2023-05-09**|**StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator**|Jiazhi Guan et.al.|[2305.05445](http://arxiv.org/abs/2305.05445)|null|\n", "2305.14359": "|**2023-05-09**|**Zero-shot personalized lip-to-speech synthesis with face image based voice control**|Zheng-Yan Sheng et.al.|[2305.14359](http://arxiv.org/abs/2305.14359)|null|\n", "2305.03713": "|**2023-09-12**|**Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos**|Ekta Prashnani et.al.|[2305.03713](http://arxiv.org/abs/2305.03713)|null|\n", "2305.02594": "|**2023-05-09**|**Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator**|Chao Xu et.al.|[2305.02594](http://arxiv.org/abs/2305.02594)|null|\n", "2305.02572": "|**2023-05-31**|**High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning**|Chao Xu et.al.|[2305.02572](http://arxiv.org/abs/2305.02572)|null|\n", "2305.00942": "|**2023-05-01**|**StyleAvatar: Real-time Photo-realistic Portrait Avatar from a Single Video**|Lizhen Wang et.al.|[2305.00942](http://arxiv.org/abs/2305.00942)|**[link](https://github.com/lizhenwangt/styleavatar)**|\n", "2305.00787": "|**2023-05-01**|**GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation**|Zhenhui Ye et.al.|[2305.00787](http://arxiv.org/abs/2305.00787)|null|\n", "2305.00521": "|**2024-02-12**|**StyleLipSync: Style-based Personalized Lip-sync Video Generation**|Taekyung Ki et.al.|[2305.00521](http://arxiv.org/abs/2305.00521)|null|\n", "2304.14471": "|**2023-04-27**|**Controllable One-Shot Face Video Synthesis With Semantic Aware Prior**|Kangning Liu et.al.|[2304.14471](http://arxiv.org/abs/2304.14471)|null|\n", "2304.12995": "|**2023-04-25**|**AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head**|Rongjie Huang et.al.|[2304.12995](http://arxiv.org/abs/2304.12995)|**[link](https://github.com/aigc-audio/audiogpt)**|\n", "2304.11113": "|**2023-04-21**|**Implicit Neural Head Synthesis via Controllable Local Deformation Fields**|Chuhan Chen et.al.|[2304.11113](http://arxiv.org/abs/2304.11113)|null|\n", "2304.10168": "|**2023-11-02**|**High-Fidelity and Freely Controllable Talking Head Video Generation**|Yue Gao et.al.|[2304.10168](http://arxiv.org/abs/2304.10168)|null|\n", "2304.08945": "|**2023-04-18**|**Audio-Driven Talking Face Generation with Diverse yet Realistic Facial Animations**|Rongliang Wu et.al.|[2304.08945](http://arxiv.org/abs/2304.08945)|null|\n", "2304.05097": "|**2023-04-11**|**One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field**|Weichuang Li et.al.|[2304.05097](http://arxiv.org/abs/2304.05097)|null|\n", "2304.03275": "|**2023-09-18**|**That's What I Said: Fully-Controllable Talking Face Generation**|Youngjoon Jang et.al.|[2304.03275](http://arxiv.org/abs/2304.03275)|null|\n", "2304.03199": "|**2023-04-06**|**Face Animation with an Attribute-Guided Diffusion Model**|Bohan Zeng et.al.|[2304.03199](http://arxiv.org/abs/2304.03199)|**[link](https://github.com/zengbohan0217/fadm)**|\n", "2304.00471": "|**2023-04-28**|**A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation**|Bo-Kyeong Kim et.al.|[2304.00471](http://arxiv.org/abs/2304.00471)|null|\n", "2304.00334": "|**2023-04-01**|**TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles**|Yifeng Ma et.al.|[2304.00334](http://arxiv.org/abs/2304.00334)|null|\n", "2303.17789": "|**2023-03-31**|**FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions**|Jin Liu et.al.|[2303.17789](http://arxiv.org/abs/2303.17789)|null|\n", "2303.17550": "|**2024-03-01**|**DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder**|Chenpeng Du et.al.|[2303.17550](http://arxiv.org/abs/2303.17550)|null|\n", "2303.17480": "|**2023-03-29**|**Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert**|Jiadong Wang et.al.|[2303.17480](http://arxiv.org/abs/2303.17480)|**[link](https://github.com/sxjdwang/talklip)**|\n", "2303.15539": "|**2023-03-27**|**OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis**|Hongyi Xu et.al.|[2303.15539](http://arxiv.org/abs/2303.15539)|null|\n", "2303.14662": "|**2023-03-26**|**OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering**|Zhiyuan Ma et.al.|[2303.14662](http://arxiv.org/abs/2303.14662)|**[link](https://github.com/theericma/otavatar)**|\n", "2303.13071": "|**2023-03-23**|**PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\\circ}$**|Sizhe An et.al.|[2303.13071](http://arxiv.org/abs/2303.13071)|null|\n", "2303.11548": "|**2023-03-26**|**Emotionally Enhanced Talking Face Generation**|Sahil Goyal et.al.|[2303.11548](http://arxiv.org/abs/2303.11548)|**[link](https://github.com/sahilg06/EmoGen)**|\n", "2303.11089": "|**2023-08-25**|**EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation**|Ziqiao Peng et.al.|[2303.11089](http://arxiv.org/abs/2303.11089)|**[link](https://github.com/psyai-net/EmoTalk_release)**|\n", "2303.09799": "|**2023-03-22**|**Style Transfer for 2D Talking Head Animation**|Trong-Thang Pham et.al.|[2303.09799](http://arxiv.org/abs/2303.09799)|**[link](https://github.com/aioz-ai/audiodrivenstyletransfer)**|\n", "2303.07697": "|**2023-03-14**|**DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions**|Geumbyeol Hwang et.al.|[2303.07697](http://arxiv.org/abs/2303.07697)|**[link](https://github.com/deepbrainai-research/koeba)**|\n", "2303.05322": "|**2023-03-09**|**Improving Few-Shot Learning for Talking Face System with TTS Data Augmentation**|Qi Chen et.al.|[2303.05322](http://arxiv.org/abs/2303.05322)|**[link](https://github.com/moon0316/t2a)**|\n", "2303.02659": "|**2023-03-05**|**Cyber Vaccine for Deepfake Immunity**|Ching-Chun Chang et.al.|[2303.02659](http://arxiv.org/abs/2303.02659)|null|\n", "2302.14337": "|**2023-05-19**|**UniFLG: Unified Facial Landmark Generator from Text or Speech**|Kentaro Mitsui et.al.|[2302.14337](http://arxiv.org/abs/2302.14337)|null|\n", "2303.08670": "|**2023-02-27**|**Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video**|Minsu Kim et.al.|[2303.08670](http://arxiv.org/abs/2303.08670)|null|\n", "2302.13469": "|**2023-02-27**|**Memory-augmented Contrastive Learning for Talking Head Generation**|Jianrong Wang et.al.|[2302.13469](http://arxiv.org/abs/2302.13469)|**[link](https://github.com/yaxinzhao97/macl)**|\n", "2302.12532": "|**2023-02-24**|**Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention**|Bin Liu et.al.|[2302.12532](http://arxiv.org/abs/2302.12532)|null|\n", "2302.08197": "|**2023-02-16**|**OPT: One-shot Pose-Controllable Talking Head Generation**|Jin Liu et.al.|[2302.08197](http://arxiv.org/abs/2302.08197)|null|\n", "2301.13430": "|**2023-01-31**|**GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis**|Zhenhui Ye et.al.|[2301.13430](http://arxiv.org/abs/2301.13430)|null|\n", "2301.06281": "|**2023-03-01**|**DPE: Disentanglement of Pose and Expression for General Video Portrait Editing**|Youxin Pang et.al.|[2301.06281](http://arxiv.org/abs/2301.06281)|**[link](https://github.com/Carlyx/DPE)**|\n", "2301.03786": "|**2023-04-20**|**DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation**|Shuai Shen et.al.|[2301.03786](http://arxiv.org/abs/2301.03786)|**[link](https://github.com/sstzal/DiffTalk)**|\n", "2301.03396": "|**2023-07-29**|**Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation**|Micha\u0142 Stypu\u0142kowski et.al.|[2301.03396](http://arxiv.org/abs/2301.03396)|null|\n", "2301.02379": "|**2023-04-03**|**CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior**|Jinbo Xing et.al.|[2301.02379](http://arxiv.org/abs/2301.02379)|**[link](https://github.com/Doubiiu/CodeTalker)**|\n", "2301.01081": "|**2023-06-10**|**StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles**|Yifeng Ma et.al.|[2301.01081](http://arxiv.org/abs/2301.01081)|**[link](https://github.com/fuxivirtualhuman/styletalk)**|\n", "2212.12137": "|**2022-12-23**|**Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing**|William Brannon et.al.|[2212.12137](http://arxiv.org/abs/2212.12137)|null|\n", "2212.08062": "|**2023-03-27**|**MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation**|Bowen Zhang et.al.|[2212.08062](http://arxiv.org/abs/2212.08062)|**[link](https://github.com/Meta-Portrait/MetaPortrait)**|\n", "2212.05005": "|**2024-03-05**|**Memories are One-to-Many Mapping Alleviators in Talking Face Generation**|Anni Tang et.al.|[2212.05005](http://arxiv.org/abs/2212.05005)|null|\n", "2212.04970": "|**2022-12-09**|**Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers**|Yasheng Sun et.al.|[2212.04970](http://arxiv.org/abs/2212.04970)|null|\n", "2212.04248": "|**2022-12-07**|**Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors**|Zhentao Yu et.al.|[2212.04248](http://arxiv.org/abs/2212.04248)|null|\n", "2211.15064": "|**2023-03-04**|**High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors**|Yunpeng Bai et.al.|[2211.15064](http://arxiv.org/abs/2211.15064)|null|\n", "2211.14758": "|**2022-11-27**|**VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild**|Kun Cheng et.al.|[2211.14758](http://arxiv.org/abs/2211.14758)|null|\n", "2211.14506": "|**2022-11-26**|**Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis**|Duomin Wang et.al.|[2211.14506](http://arxiv.org/abs/2211.14506)|**[link](https://github.com/Dorniwang/PD-FGC-inference)**|\n", "2211.12194": "|**2023-03-13**|**SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation**|Wenxuan Zhang et.al.|[2211.12194](http://arxiv.org/abs/2211.12194)|**[link](https://github.com/winfredy/sadtalker)**|\n", "2211.09809": "|**2022-12-07**|**SPACE: Speech-driven Portrait Animation with Controllable Expression**|Siddharth Gururani et.al.|[2211.09809](http://arxiv.org/abs/2211.09809)|null|\n", "2211.00987": "|**2023-04-17**|**Autoregressive GAN for Semantic Unconditional Head Motion Generation**|Louis Airale et.al.|[2211.00987](http://arxiv.org/abs/2211.00987)|**[link](https://github.com/louisbearing/unconditionalheadmotion)**|\n", "2211.00924": "|**2022-11-03**|**SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory**|Se Jin Park et.al.|[2211.00924](http://arxiv.org/abs/2211.00924)|null|\n", "2210.07055": "|**2022-10-13**|**Sparse in Space and Time: Audio-visual Synchronisation with Trainable Selectors**|Vladimir Iashin et.al.|[2210.07055](http://arxiv.org/abs/2210.07055)|**[link](https://github.com/v-iashin/sparsesync)**|\n", "2210.06877": "|**2022-10-13**|**Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar**|Aolan Sun et.al.|[2210.06877](http://arxiv.org/abs/2210.06877)|null|\n", "2210.06186": "|**2024-03-28**|**GOTCHA: Real-Time Video Deepfake Detection via Challenge-Response**|Govind Mittal et.al.|[2210.06186](http://arxiv.org/abs/2210.06186)|**[link](https://github.com/mittalgovind/GOTCHA-Deepfakes)**|\n", "2210.03692": "|**2022-10-07**|**Compressing Video Calls using Synthetic Talking Heads**|Madhav Agarwal et.al.|[2210.03692](http://arxiv.org/abs/2210.03692)|null|\n", "2210.03335": "|**2022-10-07**|**A Keypoint Based Enhancement Method for Audio Driven Free View Talking Head Synthesis**|Yichen Han et.al.|[2210.03335](http://arxiv.org/abs/2210.03335)|null|\n", "2210.02755": "|**2022-10-06**|**Audio-Visual Face Reenactment**|Madhav Agarwal et.al.|[2210.02755](http://arxiv.org/abs/2210.02755)|**[link](https://github.com/mdv3101/AVFR-Gan)**|\n", "2209.14698": "|**2022-09-29**|**Facial Landmark Predictions with Applications to Metaverse**|Qiao Han et.al.|[2209.14698](http://arxiv.org/abs/2209.14698)|**[link](https://github.com/sweatybridge/text-to-anime)**|\n", "2209.13375": "|**2022-09-27**|**StyleMask: Disentangling the Style Space of StyleGAN2 for Neural Face Reenactment**|Stella Bounareli et.al.|[2209.13375](http://arxiv.org/abs/2209.13375)|**[link](https://github.com/stelabou/stylemask)**|\n", "2209.10507": "|**2023-10-19**|**Gemino: Practical and Robust Neural Compression for Video Conferencing**|Vibhaalakshmi Sivaraman et.al.|[2209.10507](http://arxiv.org/abs/2209.10507)|null|\n", "2209.10340": "|**2022-09-21**|**FNeVR: Neural Volume Rendering for Face Animation**|Bohan Zeng et.al.|[2209.10340](http://arxiv.org/abs/2209.10340)|**[link](https://github.com/zengbohan0217/FNeVR)**|\n", "2209.08795": "|**2022-09-19**|**AutoLV: Automatic Lecture Video Generator**|Wenbin Wang et.al.|[2209.08795](http://arxiv.org/abs/2209.08795)|null|\n", "2209.08289": "|**2023-11-28**|**Continuously Controllable Facial Expression Editing in Talking Face Videos**|Zhiyao Sun et.al.|[2209.08289](http://arxiv.org/abs/2209.08289)|null|\n", "2209.04252": "|**2022-09-09**|**Talking Head from Speech Audio using a Pre-trained Image Generator**|Mohammed M. Alghamdi et.al.|[2209.04252](http://arxiv.org/abs/2209.04252)|null|\n", "2209.01320": "|**2023-03-24**|**Synthesizing Photorealistic Virtual Humans Through Cross-modal Disentanglement**|Siddarth Ravichandran et.al.|[2209.01320](http://arxiv.org/abs/2209.01320)|null|\n", "2208.13717": "|**2022-08-29**|**StableFace: Analyzing and Improving Motion Stability for Talking Face Generation**|Jun Ling et.al.|[2208.13717](http://arxiv.org/abs/2208.13717)|null|\n", "2208.10922": "|**2024-03-15**|**StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation**|Dongchan Min et.al.|[2208.10922](http://arxiv.org/abs/2208.10922)|null|\n", "2208.09796": "|**2022-10-04**|**Towards MOOCs for Lipreading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale**|Aditya Agarwal et.al.|[2208.09796](http://arxiv.org/abs/2208.09796)|null|\n", "2208.08562": "|**2022-09-07**|**Restructurable Activation Networks**|Kartikeya Bhardwaj et.al.|[2208.08562](http://arxiv.org/abs/2208.08562)|**[link](https://github.com/arm-software/ml-restructurable-activation-networks)**|\n", "2208.08118": "|**2022-08-17**|**Extreme-scale Talking-Face Video Upsampling with Audio-Visual Priors**|Sindhu B Hegde et.al.|[2208.08118](http://arxiv.org/abs/2208.08118)|**[link](https://github.com/Sindhu-Hegde/video-super-resolver)**|\n", "2208.02210": "|**2022-08-03**|**Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control**|Michail Christos Doukas et.al.|[2208.02210](http://arxiv.org/abs/2208.02210)|null|\n", "2207.11770": "|**2022-07-24**|**Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis**|Shuai Shen et.al.|[2207.11770](http://arxiv.org/abs/2207.11770)|**[link](https://github.com/sstzal/DFRF)**|\n", "2207.11094": "|**2022-07-22**|**Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos**|Panagiotis P. Filntisis et.al.|[2207.11094](http://arxiv.org/abs/2207.11094)|**[link](https://github.com/filby89/spectre)**|\n", "2207.03800": "|**2022-07-13**|**FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis**|Yongqi Wang et.al.|[2207.03800](http://arxiv.org/abs/2207.03800)|null|\n", "2206.14658": "|**2022-06-29**|**Cut Inner Layers: A Structured Pruning Strategy for Efficient U-Net GANs**|Bo-Kyeong Kim et.al.|[2206.14658](http://arxiv.org/abs/2206.14658)|null|\n", "2206.12837": "|**2022-08-02**|**Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer**|Ailin Huang et.al.|[2206.12837](http://arxiv.org/abs/2206.12837)|**[link](https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration)**|\n", "2206.07458": "|**2022-07-20**|**VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection**|Joanna Hong et.al.|[2206.07458](http://arxiv.org/abs/2206.07458)|null|\n", "2205.15278": "|**2022-09-23**|**EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model**|Xinya Ji et.al.|[2205.15278](http://arxiv.org/abs/2205.15278)|null|\n", "2205.13368": "|**2022-05-26**|**One-Shot Face Reenactment on Megapixels**|Wonjun Kang et.al.|[2205.13368](http://arxiv.org/abs/2205.13368)|null|\n", "2205.12194": "|**2022-05-24**|**Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel's Weekly Video Podcasts**|Debjoy Saha et.al.|[2205.12194](http://arxiv.org/abs/2205.12194)|**[link](https://github.com/deeplsd/merkel-podcast-corpus)**|\n", "2205.06421": "|**2022-05-13**|**Talking Face Generation with Multilingual TTS**|Hyoung-Kyu Song et.al.|[2205.06421](http://arxiv.org/abs/2205.06421)|null|\n", "2205.01155": "|**2022-05-02**|**Emotion-Controllable Generalized Talking Face Generation**|Sanjana Sinha et.al.|[2205.01155](http://arxiv.org/abs/2205.01155)|null|\n", "2205.00916": "|**2022-05-02**|**A Novel Speech-Driven Lip-Sync Model with CNN and LSTM**|Xiaohong Li et.al.|[2205.00916](http://arxiv.org/abs/2205.00916)|null|\n", "2204.14057": "|**2022-05-27**|**Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast**|Boqing Zhu et.al.|[2204.14057](http://arxiv.org/abs/2204.14057)|**[link](https://github.com/cocoxili/cmpc)**|\n", "2204.12756": "|**2022-04-27**|**Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion**|Sen Chen et.al.|[2204.12756](http://arxiv.org/abs/2204.12756)|null|\n", "2204.06180": "|**2022-04-13**|**Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions**|Zipeng Ye et.al.|[2204.06180](http://arxiv.org/abs/2204.06180)|null|\n", "2204.03083": "|**2023-05-18**|**Audio-Visual Person-of-Interest DeepFake Detection**|Davide Cozzolino et.al.|[2204.03083](http://arxiv.org/abs/2204.03083)|**[link](https://github.com/grip-unina/poi-forensics)**|\n", "2203.14512": "|**2023-02-14**|**Expressive Talking Head Video Encoding in StyleGAN2 Latent-Space**|Trevine Oorloff et.al.|[2203.14512](http://arxiv.org/abs/2203.14512)|**[link](https://github.com/trevineoorloff/Encode-in-Style)**|\n", "2203.14367": "|**2022-03-29**|**Thin-Plate Spline Motion Model for Image Animation**|Jian Zhao et.al.|[2203.14367](http://arxiv.org/abs/2203.14367)|**[link](https://github.com/yoyo-nb/thin-plate-spline-motion-model)**|\n", "2203.10117": "|**2022-11-10**|**On the role of Lip Articulation in Visual Speech Perception**|Zakaria Aldeneh et.al.|[2203.10117](http://arxiv.org/abs/2203.10117)|null|\n", "2203.07931": "|**2023-08-12**|**DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation**|Yichao Yan et.al.|[2203.07931](http://arxiv.org/abs/2203.07931)|null|\n", "2203.06605": "|**2022-03-15**|**Depth-Aware Generative Adversarial Network for Talking Head Video Generation**|Fa-Ting Hong et.al.|[2203.06605](http://arxiv.org/abs/2203.06605)|**[link](https://github.com/harlanhong/cvpr2022-dagan)**|\n", "2203.05178": "|**2022-03-10**|**An Audio-Visual Attention Based Multimodal Network for Fake Talking Face Videos Detection**|Ganglai Wang et.al.|[2203.05178](http://arxiv.org/abs/2203.05178)|null|\n", "2203.04036": "|**2022-03-17**|**StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN**|Fei Yin et.al.|[2203.04036](http://arxiv.org/abs/2203.04036)|**[link](https://github.com/FeiiYin/StyleHEAT)**|\n", "2203.03984": "|**2022-03-08**|**Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild**|Ganglai Wang et.al.|[2203.03984](http://arxiv.org/abs/2203.03984)|null|\n", "2202.12972": "|**2022-02-25**|**FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment**|Yuval Nirkin et.al.|[2202.12972](http://arxiv.org/abs/2202.12972)|null|\n", "2202.10758": "|**2022-02-22**|**Thinking the Fusion Strategy of Multi-reference Face Reenactment**|Takuya Yashima et.al.|[2202.10758](http://arxiv.org/abs/2202.10758)|null|\n", "2202.06198": "|**2023-01-23**|**Data standardization for robust lip sync**|Chun Wang et.al.|[2202.06198](http://arxiv.org/abs/2202.06198)|null|\n", "2202.00046": "|**2022-10-06**|**Finding Directions in GAN's Latent Space for Neural Face Reenactment**|Stella Bounareli et.al.|[2202.00046](http://arxiv.org/abs/2202.00046)|**[link](https://github.com/stelabou/stylegan_directions_face_reenactment)**|\n", "2201.08361": "|**2022-01-21**|**Stitch it in Time: GAN-Based Facial Editing of Real Videos**|Rotem Tzaban et.al.|[2201.08361](http://arxiv.org/abs/2201.08361)|**[link](https://github.com/rotemtzaban/STIT)**|\n", "2201.07131": "|**2022-10-21**|**Leveraging Real Talking Faces via Self-Supervision for Robust Forgery Detection**|Alexandros Haliassos et.al.|[2201.07131](http://arxiv.org/abs/2201.07131)|**[link](https://github.com/ahaliassos/RealForensics)**|\n", "2201.06260": "|**2022-01-17**|**Towards Realistic Visual Dubbing with Heterogeneous Sources**|Tianyi Xie et.al.|[2201.06260](http://arxiv.org/abs/2201.06260)|null|\n", "2201.05986": "|**2022-01-16**|**Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels**|Zipeng Ye et.al.|[2201.05986](http://arxiv.org/abs/2201.05986)|null|\n", "2201.00791": "|**2022-01-03**|**DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering**|Shunyu Yao et.al.|[2201.00791](http://arxiv.org/abs/2201.00791)|null|\n", "2112.13548": "|**2022-07-20**|**Responsive Listening Head Generation: A Benchmark Dataset and Baseline**|Mohan Zhou et.al.|[2112.13548](http://arxiv.org/abs/2112.13548)|null|\n", "2112.10098": "|**2021-12-19**|**Initiative Defense against Facial Manipulation**|Qidong Huang et.al.|[2112.10098](http://arxiv.org/abs/2112.10098)|**[link](https://github.com/shikiw/initiative-defense-for-deepfake)**|\n", "2112.02749": "|**2021-12-06**|**One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning**|Suzhen Wang et.al.|[2112.02749](http://arxiv.org/abs/2112.02749)|null|\n", "2111.03472": "|**2021-11-02**|**BiosecurID: a multimodal biometric database**|Julian Fierrez et.al.|[2111.03472](http://arxiv.org/abs/2111.03472)|null|\n", "2111.00203": "|**2021-10-30**|**Imitating Arbitrary Talking Style for Realistic Audio-DrivenTalking Face Synthesis**|Haozhe Wu et.al.|[2111.00203](http://arxiv.org/abs/2111.00203)|**[link](https://github.com/wuhaozhe/style_avatar)**|\n", "2110.13571": "|**2021-10-26**|**Emotion recognition in talking-face videos using persistent entropy and neural networks**|Eduardo Paluzo-Hidalgo et.al.|[2110.13571](http://arxiv.org/abs/2110.13571)|**[link](https://github.com/cimagroup/audiovisual-emotionrecognitionusingtda)**|\n", "2110.13384": "|**2021-10-26**|**ViDA-MAN: Visual Dialog with Digital Humans**|Tong Shen et.al.|[2110.13384](http://arxiv.org/abs/2110.13384)|null|\n", "2110.09951": "|**2021-10-19**|**Talking Head Generation with Audio and Speech Related Facial Action Units**|Sen Chen et.al.|[2110.09951](http://arxiv.org/abs/2110.09951)|null|\n", "2110.08580": "|**2021-10-16**|**Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor**|Anchit Gupta et.al.|[2110.08580](http://arxiv.org/abs/2110.08580)|null|\n", "2110.04708": "|**2021-10-12**|**Fine-grained Identity Preserving Landmark Synthesis for Face Reenactment**|Haichao Zhang et.al.|[2110.04708](http://arxiv.org/abs/2110.04708)|null|\n", "2110.05241": "|**2021-10-07**|**Streaming Transformer Transducer Based Speech Recognition Using Non-Causal Convolution**|Yangyang Shi et.al.|[2110.05241](http://arxiv.org/abs/2110.05241)|null|\n", "2109.10595": "|**2021-09-24**|**Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation**|Yuanxun Lu et.al.|[2109.10595](http://arxiv.org/abs/2109.10595)|null|\n", "2109.04991": "|**2021-09-17**|**Detection of GAN-synthesized street videos**|Omran Alamayreh et.al.|[2109.04991](http://arxiv.org/abs/2109.04991)|null|\n", "2109.02081": "|**2023-08-21**|**Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis**|Tong Sha et.al.|[2109.02081](http://arxiv.org/abs/2109.02081)|null|\n", "2108.08020": "|**2021-11-29**|**Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates**|Shenhan Qian et.al.|[2108.08020](http://arxiv.org/abs/2108.08020)|**[link](https://github.com/shenhanqian/speechdrivestemplates)**|\n", "2108.07938": "|**2021-08-18**|**FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning**|Chenxu Zhang et.al.|[2108.07938](http://arxiv.org/abs/2108.07938)|**[link](https://github.com/zhangchenxu528/FACIAL)**|\n", "2108.05650": "|**2021-08-12**|**UniFaceGAN: A Unified Framework for Temporally Consistent Facial Video Editing**|Meng Cao et.al.|[2108.05650](http://arxiv.org/abs/2108.05650)|null|\n", "2108.05080": "|**2022-03-01**|**FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset**|Hasam Khalid et.al.|[2108.05080](http://arxiv.org/abs/2108.05080)|**[link](https://github.com/dash-lab/fakeavceleb)**|\n", "2108.04325": "|**2021-08-11**|**AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person**|Xinsheng Wang et.al.|[2108.04325](http://arxiv.org/abs/2108.04325)|null|\n", "2107.12346": "|**2021-07-27**|**Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations**|Laurent Benaroya et.al.|[2107.12346](http://arxiv.org/abs/2107.12346)|null|\n", "2107.09293": "|**2021-07-20**|**Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion**|Suzhen Wang et.al.|[2107.09293](http://arxiv.org/abs/2107.09293)|**[link](https://github.com/wangsuzhen/Audio2Head)**|\n", "2107.06831": "|**2021-12-20**|**Parallel and High-Fidelity Text-to-Lip Generation**|Jinglin Liu et.al.|[2107.06831](http://arxiv.org/abs/2107.06831)|**[link](https://github.com/Dianezzy/ParaLip)**|\n", "2107.04806": "|**2021-07-10**|**Speech2Video: Cross-Modal Distillation for Speech to Video Generation**|Shijing Si et.al.|[2107.04806](http://arxiv.org/abs/2107.04806)|null|\n", "2107.03109": "|**2021-07-07**|**Egocentric Videoconferencing**|Mohamed Elgharib et.al.|[2107.03109](http://arxiv.org/abs/2107.03109)|null|\n", "2107.05548": "|**2022-03-04**|**Multi-modality Deep Restoration of Extremely Compressed Face Videos**|Xi Zhang et.al.|[2107.05548](http://arxiv.org/abs/2107.05548)|null|\n", "2106.14014": "|**2022-04-03**|**Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text**|Pulkit Tandon et.al.|[2106.14014](http://arxiv.org/abs/2106.14014)|**[link](https://github.com/tpulkit/txt2vid)**|\n", "2106.04185": "|**2021-06-08**|**LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization**|Avisek Lahiri et.al.|[2106.04185](http://arxiv.org/abs/2106.04185)|null|\n", "2104.14631": "|**2022-01-22**|**Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary**|Sibo Zhang et.al.|[2104.14631](http://arxiv.org/abs/2104.14631)|null|\n", "2104.14557": "|**2021-04-29**|**Learned Spatial Representations for Few-shot Talking-Head Synthesis**|Moustafa Meshry et.al.|[2104.14557](http://arxiv.org/abs/2104.14557)|null|\n", "2104.12051": "|**2021-04-25**|**3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head**|Qianyun Wang et.al.|[2104.12051](http://arxiv.org/abs/2104.12051)|null|\n", "2104.11116": "|**2021-04-22**|**Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation**|Hang Zhou et.al.|[2104.11116](http://arxiv.org/abs/2104.11116)|**[link](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS)**|\n", "2104.07995": "|**2021-05-07**|**Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation**|Lincheng Li et.al.|[2104.07995](http://arxiv.org/abs/2104.07995)|**[link](https://github.com/FuxiVirtualHuman/Write-a-Speaker)**|\n", "2104.07452": "|**2021-05-20**|**Audio-Driven Emotional Video Portraits**|Xinya Ji et.al.|[2104.07452](http://arxiv.org/abs/2104.07452)|null|\n", "2104.03117": "|**2021-04-07**|**Single Source One Shot Reenactment using Weighted motion From Paired Feature Points**|Soumya Tripathy et.al.|[2104.03117](http://arxiv.org/abs/2104.03117)|null|\n", "2104.03061": "|**2021-04-07**|**Everything's Talkin': Pareidolia Face Reenactment**|Linsen Song et.al.|[2104.03061](http://arxiv.org/abs/2104.03061)|**[link](https://github.com/Linsen13/EverythingTalking)**|\n", "2104.02850": "|**2021-04-07**|**LI-Net: Large-Pose Identity-Preserving Face Reenactment Network**|Jin Liu et.al.|[2104.02850](http://arxiv.org/abs/2104.02850)|null|\n", "2103.11078": "|**2021-08-19**|**AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis**|Yudong Guo et.al.|[2103.11078](http://arxiv.org/abs/2103.11078)|**[link](https://github.com/YudongGuo/AD-NeRF)**|\n", "2103.10094": "|**2021-08-23**|**KoDF: A Large-scale Korean DeepFake Detection Dataset**|Patrick Kwon et.al.|[2103.10094](http://arxiv.org/abs/2103.10094)|null|\n", "2103.03663": "|**2021-03-05**|**Real-time RGBD-based Extended Body Pose Estimation**|Renat Bashirov et.al.|[2103.03663](http://arxiv.org/abs/2103.03663)|**[link](https://github.com/rmbashirov/rgbd-kinect-pose)**|\n", "2102.09737": "|**2021-02-19**|**One Shot Audio to Animated Video Generation**|Neeraj Kumar et.al.|[2102.09737](http://arxiv.org/abs/2102.09737)|null|\n", "2103.03927": "|**2021-02-18**|**AudioVisual Speech Synthesis: A brief literature review**|Efthymios Georgiou et.al.|[2103.03927](http://arxiv.org/abs/2103.03927)|null|\n", "2102.03984": "|**2021-04-26**|**One-shot Face Reenactment Using Appearance Adaptive Normalization**|Guangming Yao et.al.|[2102.03984](http://arxiv.org/abs/2102.03984)|null|\n", "2101.10808": "|**2022-04-25**|**Fast Facial Landmark Detection and Applications: A Survey**|Kostiantyn Khabarlak et.al.|[2101.10808](http://arxiv.org/abs/2101.10808)|null|\n", "2012.08261": "|**2021-08-23**|**HeadGAN: One-shot Neural Head Synthesis and Editing**|Michail Christos Doukas et.al.|[2012.08261](http://arxiv.org/abs/2012.08261)|null|\n", "2012.07842": "|**2020-12-14**|**Robust One Shot Audio to Video Generation**|Neeraj Kumar et.al.|[2012.07842](http://arxiv.org/abs/2012.07842)|null|\n", "2012.07304": "|**2020-12-14**|**Multi Modal Adaptive Normalization for Audio to Video Generation**|Neeraj Kumar et.al.|[2012.07304](http://arxiv.org/abs/2012.07304)|null|\n", "2011.15126": "|**2021-04-02**|**One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing**|Ting-Chun Wang et.al.|[2011.15126](http://arxiv.org/abs/2011.15126)|null|\n", "2011.14695": "|**2020-11-30**|**Adaptive Compact Attention For Few-shot Video-to-video Translation**|Risheng Huang et.al.|[2011.14695](http://arxiv.org/abs/2011.14695)|null|\n", "2011.10727": "|**2020-11-21**|**Stochastic Talking Face Generation Using Latent Distribution Matching**|Ravindra Yadav et.al.|[2011.10727](http://arxiv.org/abs/2011.10727)|**[link](https://github.com/ry85/Stochastic-Talking-Face-Generation-Using-Latent-Distribution-Matching)**|\n", "2011.10688": "|**2020-11-21**|**Iterative Text-based Editing of Talking-heads Using Neural Retargeting**|Xinwei Yao et.al.|[2011.10688](http://arxiv.org/abs/2011.10688)|null|\n", "2011.04439": "|**2020-11-09**|**FACEGAN: Facial Attribute Controllable rEenactment GAN**|Soumya Tripathy et.al.|[2011.04439](http://arxiv.org/abs/2011.04439)|null|\n", "2011.01114": "|**2020-11-02**|**Facial Keypoint Sequence Generation from Audio**|Prateek Manocha et.al.|[2011.01114](http://arxiv.org/abs/2011.01114)|null|\n", "2010.13017": "|**2020-10-25**|**APB2FaceV2: Real-Time Audio-Guided Multi-Face Reenactment**|Jiangning Zhang et.al.|[2010.13017](http://arxiv.org/abs/2010.13017)|**[link](https://github.com/zhangzjn/APB2FaceV2)**|\n", "2010.02315": "|**2020-10-05**|**SMILE: Semantically-guided Multi-attribute Image and Layout Editing**|Andr\u00e9s Romero et.al.|[2010.02315](http://arxiv.org/abs/2010.02315)|**[link](https://github.com/affromero/SMILE)**|\n", "2009.05784": "|**2020-09-12**|**DualLip: A System for Joint Lip Reading and Generation**|Weicong Chen et.al.|[2009.05784](http://arxiv.org/abs/2009.05784)|null|\n", "2009.01225": "|**2020-09-02**|**Seeing wake words: Audio-visual Keyword Spotting**|Liliane Momeni et.al.|[2009.01225](http://arxiv.org/abs/2009.01225)|null|\n", "2008.13040": "|**2020-08-29**|**\"It took me almost 30 minutes to practice this\". Performance and Production Practices in Dance Challenge Videos on TikTok**|Daniel Klug et.al.|[2008.13040](http://arxiv.org/abs/2008.13040)|null|\n", "2008.11985": "|**2021-03-03**|**Estimating Uniqueness of I-Vector Representation of Human Voice**|Erkam Sinan Tandogan et.al.|[2008.11985](http://arxiv.org/abs/2008.11985)|null|\n", "2008.10010": "|**2020-08-23**|**A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild**|K R Prajwal et.al.|[2008.10010](http://arxiv.org/abs/2008.10010)|**[link](https://github.com/Rudrabha/Wav2Lip)**|\n", "2008.07783": "|**2020-09-18**|**Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks**|Guangming Yao et.al.|[2008.07783](http://arxiv.org/abs/2008.07783)|null|\n", "2008.03592": "|**2021-07-21**|**Speech Driven Talking Face Generation from a Single Image and an Emotion Condition**|Sefik Emre Eskimez et.al.|[2008.03592](http://arxiv.org/abs/2008.03592)|**[link](https://github.com/eeskimez/emotalkingface)**|\n", "2008.02098": "|**2020-08-04**|**Speaker dependent acoustic-to-articulatory inversion using real-time MRI of the vocal tract**|Tam\u00e1s G\u00e1bor Csap\u00f3 et.al.|[2008.02098](http://arxiv.org/abs/2008.02098)|**[link](https://github.com/BME-SmartLab/speech2mri)**|\n", "2008.00620": "|**2021-08-30**|**Audiovisual Speech Synthesis using Tacotron2**|Ahmed Hussen Abdelaziz et.al.|[2008.00620](http://arxiv.org/abs/2008.00620)|null|\n", "2008.01652": "|**2020-08-02**|**Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos**|Yanhui Guo et.al.|[2008.01652](http://arxiv.org/abs/2008.01652)|null|\n", "2007.08547": "|**2020-07-16**|**Talking-head Generation with Rhythmic Head Motion**|Lele Chen et.al.|[2007.08547](http://arxiv.org/abs/2007.08547)|**[link](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion)**|\n", "2007.04134": "|**2020-07-08**|**Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision**|Abhinav Shukla et.al.|[2007.04134](http://arxiv.org/abs/2007.04134)|null|\n", "2006.11610": "|**2020-06-20**|**Speaker Independent and Multilingual/Mixlingual Speech-Driven Talking Head Generation Using Phonetic Posteriorgrams**|Huirong Huang et.al.|[2006.11610](http://arxiv.org/abs/2006.11610)|null|\n", "2005.14405": "|**2021-03-20**|**Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization**|Komal Chugh et.al.|[2005.14405](http://arxiv.org/abs/2005.14405)|**[link](https://github.com/abhinavdhall/deepfake)**|\n", "2005.13616": "|**2020-05-27**|**Modality Dropout for Improved Performance-driven Talking Faces**|Ahmed Hussen Abdelaziz et.al.|[2005.13616](http://arxiv.org/abs/2005.13616)|null|\n", "2005.12318": "|**2020-05-25**|**Identity-Preserving Realistic Talking Face Generation**|Sanjana Sinha et.al.|[2005.12318](http://arxiv.org/abs/2005.12318)|null|\n", "2005.10954": "|**2020-05-22**|**Head2Head: Video-based Neural Head Synthesis**|Mohammad Rami Koujan et.al.|[2005.10954](http://arxiv.org/abs/2005.10954)|null|\n", "2005.08606": "|**2021-03-19**|**End-to-End Lip Synchronisation Based on Pattern Classification**|You Jin Kim et.al.|[2005.08606](http://arxiv.org/abs/2005.08606)|null|\n", "2005.06402": "|**2020-05-13**|**FaR-GAN for One-Shot Face Reenactment**|Hanxiang Hao et.al.|[2005.06402](http://arxiv.org/abs/2005.06402)|null|\n", "2005.03201": "|**2020-05-07**|**What comprises a good talking-head video generation?: A Survey and Benchmark**|Lele Chen et.al.|[2005.03201](http://arxiv.org/abs/2005.03201)|**[link](https://github.com/lelechen63/talking-head-generation-survey)**|\n", "2004.14569": "|**2020-04-30**|**APB2Face: Audio-guided face reenactment with auxiliary pose and blink signals**|Jiangning Zhang et.al.|[2004.14569](http://arxiv.org/abs/2004.14569)|null|\n", "2004.12992": "|**2021-02-25**|**MakeItTalk: Speaker-Aware Talking-Head Animation**|Yang Zhou et.al.|[2004.12992](http://arxiv.org/abs/2004.12992)|null|\n", "2004.05478": "|**2020-05-11**|**Dancing to the Partisan Beat: A First Analysis of Political Communication on TikTok**|Juan Carlos Medina Serrano et.al.|[2004.05478](http://arxiv.org/abs/2004.05478)|**[link](https://github.com/JuanCarlosCSE/TikTok)**|\n", "2003.13840": "|**2020-03-30**|**ActGAN: Flexible and Efficient One-shot Face Reenactment**|Ivan Kosarevych et.al.|[2003.13840](http://arxiv.org/abs/2003.13840)|null|\n", "2003.12957": "|**2020-03-29**|**Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose**|Xianfang Zeng et.al.|[2003.12957](http://arxiv.org/abs/2003.12957)|null|\n", "2003.02436": "|**2020-03-05**|**Talking-Heads Attention**|Noam Shazeer et.al.|[2003.02436](http://arxiv.org/abs/2003.02436)|**[link](https://github.com/zygmuntz/hyperband)**|\n", "2003.00418": "|**2020-03-01**|**Towards Automatic Face-to-Face Translation**|Prajwal K R et.al.|[2003.00418](http://arxiv.org/abs/2003.00418)|**[link](https://github.com/Rudrabha/LipGAN)**|\n", "2002.10137": "|**2020-03-05**|**Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose**|Ran Yi et.al.|[2002.10137](http://arxiv.org/abs/2002.10137)|**[link](https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose)**|\n", "2002.08742": "|**2020-05-04**|**Disentangled Speech Embeddings using Cross-modal Self-supervision**|Arsha Nagrani et.al.|[2002.08742](http://arxiv.org/abs/2002.08742)|null|\n", "2002.08700": "|**2021-05-05**|**A Neural Lip-Sync Framework for Synthesizing Photorealistic Virtual News Anchors**|Ruobing Zheng et.al.|[2002.08700](http://arxiv.org/abs/2002.08700)|null|\n", "1912.05833": "|**2020-02-19**|**Speech-driven facial animation using polynomial fusion of features**|Triantafyllos Kefalas et.al.|[1912.05833](http://arxiv.org/abs/1912.05833)|null|\n", "1912.05566": "|**2020-07-29**|**Neural Voice Puppetry: Audio-driven Facial Reenactment**|Justus Thies et.al.|[1912.05566](http://arxiv.org/abs/1912.05566)|**[link](https://github.com/miu200521358/NeuralVoicePuppetryMMD)**|\n", "1911.09224": "|**2019-11-21**|**FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis**|Kuangxiao Gu et.al.|[1911.09224](http://arxiv.org/abs/1911.09224)|null|\n", "1911.08139": "|**2019-11-19**|**MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets**|Sungjoo Ha et.al.|[1911.08139](http://arxiv.org/abs/1911.08139)|null|\n", "1910.12713": "|**2019-10-28**|**Few-shot Video-to-Video Synthesis**|Ting-Chun Wang et.al.|[1910.12713](http://arxiv.org/abs/1910.12713)|null|\n", "1910.08685": "|**2019-10-19**|**Real-Time Lip Sync for Live 2D Animation**|Deepali Aneja et.al.|[1910.08685](http://arxiv.org/abs/1910.08685)|**[link](https://github.com/deepalianeja/CharacterLipSync2D)**|\n", "1910.07514": "|**2019-10-16**|**Designing Style Matching Conversational Agents**|Deepali Aneja et.al.|[1910.07514](http://arxiv.org/abs/1910.07514)|null|\n", "1910.00726": "|**2019-10-02**|**Animating Face using Disentangled Audio Representations**|Gaurav Mittal et.al.|[1910.00726](http://arxiv.org/abs/1910.00726)|null|\n", "1909.08766": "|**2019-10-15**|**A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression Capabilities**|Deepali Aneja et.al.|[1909.08766](http://arxiv.org/abs/1909.08766)|**[link](https://github.com/danmcduff/AvatarSim)**|\n", "1908.11039": "|**2019-08-29**|**3D Face Pose and Animation Tracking via Eigen-Decomposition based Bayesian Approach**|Ngoc-Trung Tran et.al.|[1908.11039](http://arxiv.org/abs/1908.11039)|null|\n", "1908.07226": "|**2019-08-20**|**Prosodic Phrase Alignment for Machine Dubbing**|Alp \u00d6ktem et.al.|[1908.07226](http://arxiv.org/abs/1908.07226)|**[link](https://github.com/alpoktem/MachineDub)**|\n", "1908.05932": "|**2019-08-16**|**FSGAN: Subject Agnostic Face Swapping and Reenactment**|Yuval Nirkin et.al.|[1908.05932](http://arxiv.org/abs/1908.05932)|**[link](https://github.com/YuvalNirkin/fsgan)**|\n", "1908.03251": "|**2019-08-05**|**One-shot Face Reenactment**|Yunxuan Zhang et.al.|[1908.03251](http://arxiv.org/abs/1908.03251)|**[link](https://github.com/bj80heyue/Learning_One_Shot_Face_Reenactment)**|\n", "1907.12918": "|**2019-10-09**|**EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos**|Haipeng Zeng et.al.|[1907.12918](http://arxiv.org/abs/1907.12918)|null|\n", "1906.06337": "|**2019-06-14**|**Realistic Speech-Driven Facial Animation with GANs**|Konstantinos Vougioukas et.al.|[1906.06337](http://arxiv.org/abs/1906.06337)|null|\n", "1906.01524": "|**2019-06-04**|**Text-based Editing of Talking-head Video**|Ohad Fried et.al.|[1906.01524](http://arxiv.org/abs/1906.01524)|null|\n", "1905.11805": "|**2020-05-16**|**FReeNet: Multi-Identity Face Reenactment**|Jiangning Zhang et.al.|[1905.11805](http://arxiv.org/abs/1905.11805)|null|\n", "1905.08233": "|**2019-09-25**|**Few-Shot Adversarial Learning of Realistic Neural Talking Head Models**|Egor Zakharov et.al.|[1905.08233](http://arxiv.org/abs/1905.08233)|null|\n", "1905.03820": "|**2019-05-09**|**Hierarchical Cross-Modal Talking Face Generationwith Dynamic Pixel-Wise Loss**|Lele Chen et.al.|[1905.03820](http://arxiv.org/abs/1905.03820)|**[link](https://github.com/lelechen63/ATVGnet)**|\n", "1904.01909": "|**2020-01-17**|**ICface: Interpretable and Controllable Face Reenactment Using GANs**|Soumya Tripathy et.al.|[1904.01909](http://arxiv.org/abs/1904.01909)|null|\n", "1812.08685": "|**2018-12-20**|**DeepFakes: a New Threat to Face Recognition? Assessment and Detection**|Pavel Korshunov et.al.|[1812.08685](http://arxiv.org/abs/1812.08685)|null|\n", "1812.06589": "|**2020-05-13**|**Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning**|Hao Zhu et.al.|[1812.06589](http://arxiv.org/abs/1812.06589)|null|\n", "1812.02088": "|**2018-11-16**|**Influence of visual cues on head and eye movements during listening tasks in multi-talker audiovisual environments with animated characters**|Maartje M. E. Hendrikse et.al.|[1812.02088](http://arxiv.org/abs/1812.02088)|null|\n", "1811.00342": "|**2018-11-22**|**Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos**|Ying Tai et.al.|[1811.00342](http://arxiv.org/abs/1811.00342)|**[link](https://github.com/tyshiwo/FHR_alignment)**|\n", "1809.02108": "|**2018-12-22**|**Deep Audio-Visual Speech Recognition**|Triantafyllos Afouras et.al.|[1809.02108](http://arxiv.org/abs/1809.02108)|null|\n", "1808.06250": "|**2018-08-19**|**Dynamic Temporal Alignment of Speech to Lips**|Tavi Halperin et.al.|[1808.06250](http://arxiv.org/abs/1808.06250)|**[link](https://github.com/tavihalperin/AV-sync)**|\n", "1807.11079": "|**2018-07-29**|**ReenactGAN: Learning to Reenact Faces via Boundary Transfer**|Wayne Wu et.al.|[1807.11079](http://arxiv.org/abs/1807.11079)|**[link](https://github.com/wywu/ReenactGAN)**|\n", "1807.07860": "|**2019-04-23**|**Talking Face Generation by Adversarially Disentangled Audio-Visual Representation**|Hang Zhou et.al.|[1807.07860](http://arxiv.org/abs/1807.07860)|null|\n", "1805.09313": "|**2018-07-19**|**End-to-End Speech-Driven Facial Animation with Temporal GANs**|Konstantinos Vougioukas et.al.|[1805.09313](http://arxiv.org/abs/1805.09313)|null|\n", "1805.00833": "|**2018-07-26**|**Learnable PINs: Cross-Modal Embeddings for Person Identity**|Arsha Nagrani et.al.|[1805.00833](http://arxiv.org/abs/1805.00833)|null|\n", "1804.04786": "|**2019-07-25**|**Talking Face Generation by Conditional Recurrent Adversarial Network**|Yang Song et.al.|[1804.04786](http://arxiv.org/abs/1804.04786)|**[link](https://github.com/susanqq/Talking_Face_Generation)**|\n", "1803.09803": "|**2018-04-23**|**Generating Talking Face Landmarks from Speech**|Sefik Emre Eskimez et.al.|[1803.09803](http://arxiv.org/abs/1803.09803)|null|\n", "1803.07716": "|**2018-03-28**|**Generative Adversarial Talking Head: Bringing Portraits to Life with a Weakly Supervised Neural Network**|Hai X. Pham et.al.|[1803.07716](http://arxiv.org/abs/1803.07716)|null|\n", "1803.07461": "|**2018-03-20**|**Speech-Driven Facial Reenactment Using Conditional Generative Adversarial Networks**|Seyed Ali Jalalifar et.al.|[1803.07461](http://arxiv.org/abs/1803.07461)|null|\n", "1801.01442": "|**2017-12-06**|**ObamaNet: Photo-realistic lip-sync from text**|Rithesh Kumar et.al.|[1801.01442](http://arxiv.org/abs/1801.01442)|null|\n", "1707.06830": "|**2017-07-21**|**Multichannel Attention Network for Analyzing Visual Behavior in Public Speaking**|Rahul Sharma et.al.|[1707.06830](http://arxiv.org/abs/1707.06830)|null|\n", "1705.02966": "|**2017-07-18**|**You said that?**|Joon Son Chung et.al.|[1705.02966](http://arxiv.org/abs/1705.02966)|null|\n", "1611.05358": "|**2017-01-30**|**Lip Reading Sentences in the Wild**|Joon Son Chung et.al.|[1611.05358](http://arxiv.org/abs/1611.05358)|**[link](https://github.com/parambadiger/Lip-Reading)**|\n", "1610.09380": "|**2016-10-28**|**Galaxy gas as obscurer: II. Separating the galaxy-scale and nuclear obscurers of Active Galactic Nuclei**|Johannes Buchner et.al.|[1610.09380](http://arxiv.org/abs/1610.09380)|**[link](https://github.com/JohannesBuchner/LightRayRider)**|\n", "1602.02651": "|**2016-02-08**|**Automatic Face Reenactment**|Pablo Garrido et.al.|[1602.02651](http://arxiv.org/abs/1602.02651)|null|\n", "1409.1411": "|**2014-09-03**|**Visual Speech Recognition**|Ahmad B. A. Hassanat et.al.|[1409.1411](http://arxiv.org/abs/1409.1411)|null|\n", "1201.4080": "|**2012-01-19**|**Progress in animation of an EMA-controlled tongue model for acoustic-visual speech synthesis**|Ingmar Steiner et.al.|[1201.4080](http://arxiv.org/abs/1201.4080)|null|\n", "0708.3740": "|**2007-08-28**|**Plate-forme Magicien d'Oz pour l'\u00e9tude de l'apport des ACAs \u00e0 l'interaction**|J\u00e9r\u00f4me Simonin et.al.|[0708.3740](http://arxiv.org/abs/0708.3740)|null|\n", "cs/9812006": "|**1998-12-05**|**A High Quality Text-To-Speech System Composed of Multiple Neural Networks**|Orhan Karaali et.al.|[cs/9812006](http://arxiv.org/abs/cs/9812006)|null|\n", "cmp-lg/9807012": "|**1998-07-31**|**Character design for soccer commmentary**|Kim Binsted et.al.|[cmp-lg/9807012](http://arxiv.org/abs/cmp-lg/9807012)|null|\n", "2401.06126": "|**2024-01-11**|**Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors**|Jack Saunders et.al.|[2401.06126](http://arxiv.org/abs/2401.06126)|null|\n", "2311.01811": "|**2024-01-12**|**DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder**|Tao Liu et.al.|[2311.01811](http://arxiv.org/abs/2311.01811)|null|\n", "2401.03476": "|**2024-01-07**|**Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness**|Sicheng Yang et.al.|[2401.03476](http://arxiv.org/abs/2401.03476)|null|\n", "2310.07969": "|**2023-10-12**|**CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity**|Abdullah Hayajneh et.al.|[2310.07969](http://arxiv.org/abs/2310.07969)|**[link](https://github.com/abdullah-tamu/CleftGAN)**|\n", "2312.10877": "|**2023-12-18**|**Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation**|Hui Fu et.al.|[2312.10877](http://arxiv.org/abs/2312.10877)|null|\n", "2312.09750": "|**2023-12-15**|**Attention-Based VR Facial Animation with Visual Mouth Camera Guidance for Immersive Telepresence Avatars**|Andre Rochow et.al.|[2312.09750](http://arxiv.org/abs/2312.09750)|null|\n", "2312.06530": "|**2023-12-11**|**Study of Non-Verbal Behavior in Conversational Agents**|Camila Vicari Maccari et.al.|[2312.06530](http://arxiv.org/abs/2312.06530)|null|\n", "2312.04369": "|**2023-12-08**|**SingingHead: A Large-scale 4D Dataset for Singing Head Animation**|Sijing Wu et.al.|[2312.04369](http://arxiv.org/abs/2312.04369)|null|\n", "2312.03775": "|**2023-12-20**|**FAAC: Facial Animation Generation with Anchor Frame and Conditional Control for Superior Fidelity and Editability**|Linze Li et.al.|[2312.03775](http://arxiv.org/abs/2312.03775)|null|\n", "2312.00870": "|**2023-12-01**|**3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing**|Balamurugan Thambiraja et.al.|[2312.00870](http://arxiv.org/abs/2312.00870)|null|\n", "2311.16565": "|**2023-12-02**|**DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D Face Diffuser**|Peng Chen et.al.|[2311.16565](http://arxiv.org/abs/2311.16565)|null|\n", "2311.04766": "|**2023-11-13**|**DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation**|Guinan Su et.al.|[2311.04766](http://arxiv.org/abs/2311.04766)|null|\n", "2310.20240": "|**2023-10-31**|**Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape**|Wei Zhao et.al.|[2310.20240](http://arxiv.org/abs/2310.20240)|null|\n", "2310.17011": "|**2023-10-25**|**Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control**|Elif Bozkurt et.al.|[2310.17011](http://arxiv.org/abs/2310.17011)|null|\n", "2310.07236": "|**2024-01-08**|**AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive Speech-Driven 3D Facial Animation**|Liyang Chen et.al.|[2310.07236](http://arxiv.org/abs/2310.07236)|null|\n", "2309.11306": "|**2023-09-20**|**FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion**|Stefan Stan et.al.|[2309.11306](http://arxiv.org/abs/2309.11306)|**[link](https://github.com/uuembodiedsocialai/FaceDiffuser)**|\n", "2308.14448": "|**2023-09-11**|**ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment**|Yicheng Zhong et.al.|[2308.14448](http://arxiv.org/abs/2308.14448)|null|\n", "2310.05934": "|**2023-08-23**|**DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion**|Se Jin Park et.al.|[2310.05934](http://arxiv.org/abs/2310.05934)|null|\n", "2308.06076": "|**2023-08-11**|**Versatile Face Animator: Driving Arbitrary 3D Facial Avatar in RGBD Space**|Haoyu Wang et.al.|[2308.06076](http://arxiv.org/abs/2308.06076)|**[link](https://github.com/why986/VFA)**|\n", "2307.01468": "|**2023-07-04**|**Generating Animatable 3D Cartoon Faces from Single Portraits**|Chuanyu Pan et.al.|[2307.01468](http://arxiv.org/abs/2307.01468)|null|\n", "2306.10006": "|**2023-09-01**|**Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances**|Wolfgang Paier et.al.|[2306.10006](http://arxiv.org/abs/2306.10006)|null|\n", "2305.10589": "|**2023-05-17**|**INCLG: Inpainting for Non-Cleft Lip Generation with a Multi-Task Image Processing Network**|Shuang Chen et.al.|[2305.10589](http://arxiv.org/abs/2305.10589)|null|\n", "2305.08854": "|**2023-08-30**|**Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models**|Antoni Bigata Casademunt et.al.|[2305.08854](http://arxiv.org/abs/2305.08854)|**[link](https://github.com/antonibigata/Laughing-Matters)**|\n", "2305.03216": "|**2023-08-10**|**Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution**|Hyojoon Park et.al.|[2305.03216](http://arxiv.org/abs/2305.03216)|null|\n", "2304.12051": "|**2023-04-24**|**VR Facial Animation for Immersive Telepresence Avatars**|Andre Rochow et.al.|[2304.12051](http://arxiv.org/abs/2304.12051)|null|\n", "2304.02814": "|**2023-04-06**|**4D Agnostic Real-Time Facial Animation Pipeline for Desktop Scenarios**|Wei Chen et.al.|[2304.02814](http://arxiv.org/abs/2304.02814)|null|\n", "2304.03117": "|**2023-04-01**|**DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance**|Longwen Zhang et.al.|[2304.03117](http://arxiv.org/abs/2304.03117)|null|\n", "2303.09797": "|**2023-12-13**|**MMFace4D: A Large-Scale Multi-Modal 4D Face Dataset for Audio-Driven 3D Face Animation**|Haozhe Wu et.al.|[2303.09797](http://arxiv.org/abs/2303.09797)|null|\n", "2303.06370": "|**2023-03-26**|**Distributed Solution of the Inverse Rig Problem in Blendshape Facial Animation**|Stevo Rackovi\u0107 et.al.|[2303.06370](http://arxiv.org/abs/2303.06370)|null|\n", "2303.05416": "|**2023-03-09**|**FaceXHuBERT: Text-less Speech-driven E(X)pressive 3D Facial Animation Synthesis Using Self-Supervised Speech Representation Learning**|Kazi Injamamul Haque et.al.|[2303.05416](http://arxiv.org/abs/2303.05416)|**[link](https://github.com/galib360/facexhubert)**|\n", "2303.03988": "|**2023-03-07**|**DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video**|Zhimeng Zhang et.al.|[2303.03988](http://arxiv.org/abs/2303.03988)|**[link](https://github.com/MRzzm/DINet)**|\n", "2302.04843": "|**2023-03-27**|**Accurate and Interpretable Solution of the Inverse Rig for Realistic Blendshape Models with Quadratic Corrective Terms**|Stevo Rackovi\u0107 et.al.|[2302.04843](http://arxiv.org/abs/2302.04843)|null|\n", "2301.06059": "|**2023-01-15**|**Learning Audio-Driven Viseme Dynamics for 3D Face Animation**|Linchao Bao et.al.|[2301.06059](http://arxiv.org/abs/2301.06059)|null|\n", "2301.02008": "|**2024-01-04**|**Expressive Speech-driven Facial Animation with controllable emotions**|Yutong Chen et.al.|[2301.02008](http://arxiv.org/abs/2301.02008)|**[link](https://github.com/on1262/facialanimation)**|\n", "2301.00023": "|**2022-12-30**|**Imitator: Personalized Speech-driven 3D Facial Animation**|Balamurugan Thambiraja et.al.|[2301.00023](http://arxiv.org/abs/2301.00023)|null|\n", "2212.14784": "|**2023-01-20**|**Neural Volumetric Blendshapes: Computationally Efficient Physics-Based Facial Blendshapes**|Nicolas Wagner et.al.|[2212.14784](http://arxiv.org/abs/2212.14784)|null|\n", "2211.16710": "|**2022-11-30**|**Extracting Semantic Knowledge from GANs with Unsupervised Learning**|Jianjin Xu et.al.|[2211.16710](http://arxiv.org/abs/2211.16710)|null|\n", "2208.01149": "|**2022-08-01**|**A Feasibility Study on Image Inpainting for Non-cleft Lip Generation from Patients with Cleft Lip**|Shuang Chen et.al.|[2208.01149](http://arxiv.org/abs/2208.01149)|**[link](https://github.com/chrischen1023/nclg-mt)**|\n", "2401.08049": "|**2024-01-16**|**EmoTalker: Emotionally Editable Talking Face Generation via Diffusion Model**|Bingyuan Zhang et.al.|[2401.08049](http://arxiv.org/abs/2401.08049)|null|\n", "2401.08503": "|**2024-03-23**|**Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**|Zhenhui Ye et.al.|[2401.08503](http://arxiv.org/abs/2401.08503)|**[link](https://github.com/yerfor/Real3DPortrait)**|\n", "2401.08655": "|**2024-01-25**|**SAiD: Speech-driven Blendshape Facial Animation with Diffusion**|Inkyu Park et.al.|[2401.08655](http://arxiv.org/abs/2401.08655)|**[link](https://github.com/yunik1004/said)**|\n", "2401.10113": "|**2024-01-18**|**Exposing Lip-syncing Deepfakes from Mouth Inconsistencies**|Soumyya Kanti Datta et.al.|[2401.10113](http://arxiv.org/abs/2401.10113)|null|\n", "2401.11002": "|**2024-01-19**|**Fast Registration of Photorealistic Avatars for VR Facial Animation**|Chaitanya Patel et.al.|[2401.11002](http://arxiv.org/abs/2401.11002)|null|\n", "2401.12568": "|**2024-01-23**|**NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis**|Chongke Bi et.al.|[2401.12568](http://arxiv.org/abs/2401.12568)|null|\n", "2401.14861": "|**2024-01-26**|**Implicit Neural Representation for Physics-driven Actuated Soft Bodies**|Lingchen Yang et.al.|[2401.14861](http://arxiv.org/abs/2401.14861)|null|\n", "2401.15687": "|**2024-01-30**|**Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance**|Qingcheng Zhao et.al.|[2401.15687](http://arxiv.org/abs/2401.15687)|null|\n", "2401.15668": "|**2024-01-28**|**Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes**|Weifeng Liu et.al.|[2401.15668](http://arxiv.org/abs/2401.15668)|**[link](https://github.com/aaroncomo/lipfd)**|\n", "2401.15414": "|**2024-01-27**|**An Implicit Physical Face Model Driven by Expression and Style**|Lingchen Yang et.al.|[2401.15414](http://arxiv.org/abs/2401.15414)|null|\n", "2402.01422": "|**2024-02-02**|**EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face Generation**|Guanwen Feng et.al.|[2402.01422](http://arxiv.org/abs/2402.01422)|null|\n", "2402.03553": "|**2024-02-05**|**One-shot Neural Face Reenactment via Finding Directions in GAN's Latent Space**|Stella Bounareli et.al.|[2402.03553](http://arxiv.org/abs/2402.03553)|null|\n", "2402.05712": "|**2024-02-08**|**DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer**|Zhiyuan Ma et.al.|[2402.05712](http://arxiv.org/abs/2402.05712)|**[link](https://github.com/theericma/diffspeaker)**|\n", "2402.12636": "|**2024-02-21**|**StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing**|Gaoxiang Cong et.al.|[2402.12636](http://arxiv.org/abs/2402.12636)|null|\n", "2402.13724": "|**2024-02-21**|**Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters**|Zechen Bai et.al.|[2402.13724](http://arxiv.org/abs/2402.13724)|**[link](https://github.com/showlab/byoc)**|\n", "2402.16124": "|**2024-02-25**|**AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation**|Yasheng Sun et.al.|[2402.16124](http://arxiv.org/abs/2402.16124)|null|\n", "2402.17485": "|**2024-02-27**|**EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions**|Linrui Tian et.al.|[2402.17485](http://arxiv.org/abs/2402.17485)|null|\n", "2402.17364": "|**2024-02-27**|**Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis**|Zicheng Zhang et.al.|[2402.17364](http://arxiv.org/abs/2402.17364)|**[link](https://github.com/zhangzc21/dyntet)**|\n", "2402.16599": "|**2024-02-26**|**Resolution-Agnostic Neural Compression for High-Fidelity Portrait Video Conferencing via Implicit Radiance Fields**|Yifei Li et.al.|[2402.16599](http://arxiv.org/abs/2402.16599)|null|\n", "2402.18122": "|**2024-03-02**|**G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment**|Juan Zhang et.al.|[2402.18122](http://arxiv.org/abs/2402.18122)|null|\n", "2402.18092": "|**2024-02-28**|**Context-aware Talking Face Video Generation**|Meidai Xuanyuan et.al.|[2402.18092](http://arxiv.org/abs/2402.18092)|null|\n", "2402.19477": "|**2024-02-29**|**Learning a Generalized Physical Face Model From Data**|Lingchen Yang et.al.|[2402.19477](http://arxiv.org/abs/2402.19477)|null|\n", "2403.01901": "|**2024-04-01**|**FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio**|Chao Xu et.al.|[2403.01901](http://arxiv.org/abs/2403.01901)|**[link](https://github.com/modelscope/facechain)**|\n", "2403.06421": "|**2024-03-11**|**A Comparative Study of Perceptual Quality Metrics for Audio-driven Talking Head Videos**|Weixia Zhang et.al.|[2403.06421](http://arxiv.org/abs/2403.06421)|**[link](https://github.com/zwx8981/adth-qa)**|\n", "2403.06375": "|**2024-03-12**|**FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization**|Shuai Tan et.al.|[2403.06375](http://arxiv.org/abs/2403.06375)|null|\n", "2403.06365": "|**2024-03-12**|**Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style**|Shuai Tan et.al.|[2403.06365](http://arxiv.org/abs/2403.06365)|null|\n", "2403.06363": "|**2024-03-13**|**Say Anything with Any Style**|Shuai Tan et.al.|[2403.06363](http://arxiv.org/abs/2403.06363)|null|\n", "2403.11700": "|**2024-03-22**|**Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing**|Juan Zhang et.al.|[2403.11700](http://arxiv.org/abs/2403.11700)|null|\n", "2403.10942": "|**2024-03-19**|**ScanTalk: 3D Talking Heads from Unregistered Scans**|Federico Nocentini et.al.|[2403.10942](http://arxiv.org/abs/2403.10942)|null|\n", "2403.12886": "|**2024-03-19**|**EmoVOCA: Speech-Driven Emotional 3D Talking Heads**|Federico Nocentini et.al.|[2403.12886](http://arxiv.org/abs/2403.12886)|null|\n", "2403.15227": "|**2024-03-22**|**LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example**|Soyeon Yoon et.al.|[2403.15227](http://arxiv.org/abs/2403.15227)|**[link](https://github.com/kwanyun/LeGO_code)**|\n", "2403.16510": "|**2024-03-25**|**Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework**|Ziyao Huang et.al.|[2403.16510](http://arxiv.org/abs/2403.16510)|**[link](https://github.com/ictmcg/make-your-anchor)**|\n", "2403.15944": "|**2024-03-23**|**Adaptive Super Resolution For One-Shot Talking-Head Generation**|Luchuan Song et.al.|[2403.15944](http://arxiv.org/abs/2403.15944)|**[link](https://github.com/songluchuan/adasr-talkinghead)**|\n", "2403.15931": "|**2024-03-27**|**X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention**|You Xie et.al.|[2403.15931](http://arxiv.org/abs/2403.15931)|null|\n", "2403.17883": "|**2024-03-26**|**Superior and Pragmatic Talking Face Generation with Teacher-Student Framework**|Chao Liang et.al.|[2403.17883](http://arxiv.org/abs/2403.17883)|null|\n", "2403.17881": "|**2024-04-09**|**Deepfake Generation and Detection: A Benchmark and Survey**|Gan Pei et.al.|[2403.17881](http://arxiv.org/abs/2403.17881)|**[link](https://github.com/flyingby/awesome-deepfake-generation-and-detection)**|\n", "2403.17694": "|**2024-03-26**|**AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation**|Huawei Wei et.al.|[2403.17694](http://arxiv.org/abs/2403.17694)|**[link](https://github.com/scutzzj/aniportrait)**|\n", "2403.17217": "|**2024-03-25**|**DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment**|Stella Bounareli et.al.|[2403.17217](http://arxiv.org/abs/2403.17217)|null|\n", "2403.17213": "|**2024-03-25**|**AnimateMe: 4D Facial Expressions via Diffusion Models**|Dimitrios Gerogiannis et.al.|[2403.17213](http://arxiv.org/abs/2403.17213)|null|\n", "2403.19144": "|**2024-03-28**|**MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation**|Seyeon Kim et.al.|[2403.19144](http://arxiv.org/abs/2403.19144)|**[link](https://github.com/KU-CVLAB/MoDiTalker)**|\n", "2403.20153": "|**2024-03-29**|**Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior**|Jaehoon Ko et.al.|[2403.20153](http://arxiv.org/abs/2403.20153)|**[link](https://github.com/KU-CVLAB/Talk3D)**|\n", "2403.19920": "|**2024-04-03**|**MI-NeRF: Learning a Single Face NeRF from Multiple Identities**|Aggelina Chatziagapi et.al.|[2403.19920](http://arxiv.org/abs/2403.19920)|null|\n", "2404.01647": "|**2024-04-02**|**EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis**|Shuai Tan et.al.|[2404.01647](http://arxiv.org/abs/2404.01647)|null|\n", "2404.00636": "|**2024-04-02**|**Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation**|Taekyung Ki et.al.|[2404.00636](http://arxiv.org/abs/2404.00636)|null|\n", "2404.05680": "|**2024-04-08**|**SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation**|Heyuan Li et.al.|[2404.05680](http://arxiv.org/abs/2404.05680)|null|\n", "2404.04924": "|**2024-04-07**|**GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets**|Dongjing Shan et.al.|[2404.04924](http://arxiv.org/abs/2404.04924)|null|\n", "2404.09736": "|**2024-04-15**|**FSRT: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance, Head-pose, and Facial Expression Features**|Andre Rochow et.al.|[2404.09736](http://arxiv.org/abs/2404.09736)|null|\n", "2404.09003": "|**2024-04-13**|**THQA: A Perceptual Quality Assessment Database for Talking Heads**|Yingjie Zhou et.al.|[2404.09003](http://arxiv.org/abs/2404.09003)|**[link](https://github.com/zyj-2000/thqa)**|\n", "2404.10667": "|**2024-04-16**|**VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time**|Sicheng Xu et.al.|[2404.10667](http://arxiv.org/abs/2404.10667)|null|\n", "2309.08408": "|**2023-09-15**|**Audio-Visual Active Speaker Extraction for Sparsely Overlapped Multi-talker Speech**|Junjie Li et.al.|[2309.08408](http://arxiv.org/abs/2309.08408)|**[link](https://github.com/mrjunjieli/activeextract)**|\n", "2309.06511": "|**2023-09-12**|**DF-TransFusion: Multimodal Deepfake Detection via Lip-Audio Cross-Attention and Facial Self-Attention**|Aaditya Kharel et.al.|[2309.06511](http://arxiv.org/abs/2309.06511)|null|\n", "2307.09323": "|**2023-08-24**|**Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis**|Jiahe Li et.al.|[2307.09323](http://arxiv.org/abs/2307.09323)|**[link](https://github.com/fictionarry/er-nerf)**|\n", "2306.07476": "|**2023-06-13**|**AniFaceDrawing: Anime Portrait Exploration during Your Sketching**|Zhengyu Huang et.al.|[2306.07476](http://arxiv.org/abs/2306.07476)|null|\n", "2212.13810": "|**2022-12-28**|**All's well that FID's well? Result quality and metric scores in GAN models for lip-sychronization tasks**|Carina Geldhauser et.al.|[2212.13810](http://arxiv.org/abs/2212.13810)|null|\n", "2211.12368": "|**2022-11-22**|**Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial Decomposition**|Jiaxiang Tang et.al.|[2211.12368](http://arxiv.org/abs/2211.12368)|null|\n", "2211.06627": "|**2023-03-22**|**MARLIN: Masked Autoencoder for facial video Representation LearnINg**|Zhixi Cai et.al.|[2211.06627](http://arxiv.org/abs/2211.06627)|**[link](https://github.com/ControlNet/MARLIN)**|\n", "2207.13530": "|**2022-07-27**|**A Hybrid Deep Animation Codec for Low-bitrate Video Conferencing**|Goluck Konuko et.al.|[2207.13530](http://arxiv.org/abs/2207.13530)|null|\n", "2207.00974": "|**2022-07-20**|**NARRATE: A Normal Assisted Free-View Portrait Stylizer**|Youjia Wang et.al.|[2207.00974](http://arxiv.org/abs/2207.00974)|null|\n", "2206.04523": "|**2022-06-09**|**Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos**|Alexander Waibel et.al.|[2206.04523](http://arxiv.org/abs/2206.04523)|null|\n", "2205.15573": "|**2022-05-31**|**Text/Speech-Driven Full-Body Animation**|Wenlin Zhuang et.al.|[2205.15573](http://arxiv.org/abs/2205.15573)|null|\n", "2205.04289": "|**2023-03-27**|**A Majorization-Minimization Based Method for Nonconvex Inverse Rig Problems in Facial Animation: Algorithm Derivation**|Stevo Rackovi\u0107 et.al.|[2205.04289](http://arxiv.org/abs/2205.04289)|null|\n", "2203.16224": "|**2022-03-30**|**End to End Lip Synchronization with a Temporal AutoEncoder**|Yoav Shalev et.al.|[2203.16224](http://arxiv.org/abs/2203.16224)|**[link](https://github.com/itsyoavshalev/end-to-end-lip-synchronization-with-a-temporal-autoencoder)**|\n", "2203.08765": "|**2022-03-16**|**Efficient conditioned face animation using frontally-viewed embedding**|Maxime Oquab et.al.|[2203.08765](http://arxiv.org/abs/2203.08765)|null|\n", "2112.05329": "|**2022-03-17**|**FaceFormer: Speech-Driven 3D Facial Animation with Transformers**|Yingruo Fan et.al.|[2112.05329](http://arxiv.org/abs/2112.05329)|**[link](https://github.com/EvelynFan/FaceFormer)**|\n", "2112.02214": "|**2021-12-07**|**Joint Audio-Text Model for Expressive Speech-Driven 3D Facial Animation**|Yingruo Fan et.al.|[2112.02214](http://arxiv.org/abs/2112.02214)|null|\n", "2111.09771": "|**2022-04-06**|**Transformer-S2A: Robust and Efficient Speech-to-Animation**|Liyang Chen et.al.|[2111.09771](http://arxiv.org/abs/2111.09771)|null|\n", "2111.02751": "|**2021-11-04**|**FEAFA+: An Extended Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation**|Wei Gan et.al.|[2111.02751](http://arxiv.org/abs/2111.02751)|null|\n", "2109.08356": "|**2021-09-20**|**Accurate, Interpretable, and Fast Animation: An Iterative, Sparse, and Nonconvex Approach**|Stevo Rackovic et.al.|[2109.08356](http://arxiv.org/abs/2109.08356)|null|\n", "2109.08061": "|**2021-10-22**|**Invertible Frowns: Video-to-Video Facial Emotion Translation**|Ian Magnusson et.al.|[2109.08061](http://arxiv.org/abs/2109.08061)|null|\n", "2108.07949": "|**2021-08-18**|**DeepFake MNIST+: A DeepFake Facial Animation Dataset**|Jiajun Huang et.al.|[2108.07949](http://arxiv.org/abs/2108.07949)|**[link](https://github.com/huangjiadidi/DeepFakeMnist)**|\n", "2106.07150": "|**2022-01-24**|**Selective Listening by Synchronizing Speech with Lips**|Zexu Pan et.al.|[2106.07150](http://arxiv.org/abs/2106.07150)|**[link](https://github.com/zexupan/reentry)**|\n", "2104.08223": "|**2022-05-20**|**MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement**|Alexander Richard et.al.|[2104.08223](http://arxiv.org/abs/2104.08223)|**[link](https://github.com/facebookresearch/meshtalk)**|\n", "2011.03530": "|**2020-11-06**|**Large-scale multilingual audio visual dubbing**|Yi Yang et.al.|[2011.03530](http://arxiv.org/abs/2011.03530)|null|\n", "2010.05655": "|**2020-10-12**|**Intuitive Facial Animation Editing Based On A Generative RNN Framework**|Elo\u00efse Berson et.al.|[2010.05655](http://arxiv.org/abs/2010.05655)|null|\n", "2010.00560": "|**2020-10-05**|**Dynamic Facial Asset and Rig Generation from a Single Scan**|Jiaman Li et.al.|[2010.00560](http://arxiv.org/abs/2010.00560)|null|\n", "2009.09354": "|**2020-09-20**|**An Improved Approach of Intention Discovery with Machine Learning for POMDP-based Dialogue Management**|Ruturaj Raval et.al.|[2009.09354](http://arxiv.org/abs/2009.09354)|null|\n", "2008.05023": "|**2020-08-11**|**Audio- and Gaze-driven Facial Animation of Codec Avatars**|Alexander Richard et.al.|[2008.05023](http://arxiv.org/abs/2008.05023)|null|\n", "2008.01332": "|**2020-08-04**|**Real-Time Cleaning and Refinement of Facial Animation Signals**|Elo\u00efse Berson et.al.|[2008.01332](http://arxiv.org/abs/2008.01332)|null|\n", "2007.09367": "|**2020-07-18**|**A Robust Interactive Facial Animation Editing System**|Elo\u00efse Berson et.al.|[2007.09367](http://arxiv.org/abs/2007.09367)|null|\n", "2007.03780": "|**2021-08-06**|**SofGAN: A Portrait Image Generator with Dynamic Styling**|Anpei Chen et.al.|[2007.03780](http://arxiv.org/abs/2007.03780)|**[link](https://github.com/apchenstu/sofgan)**|\n", "2003.11038": "|**2020-07-20**|**Deformable Style Transfer**|Sunnie S. Y. Kim et.al.|[2003.11038](http://arxiv.org/abs/2003.11038)|**[link](https://github.com/sunniesuhyoung/DST)**|\n", "2003.06211": "|**2020-03-26**|**High-Accuracy Facial Depth Models derived from 3D Synthetic Data**|Faisal Khan et.al.|[2003.06211](http://arxiv.org/abs/2003.06211)|null|\n", "1909.02518": "|**2019-09-06**|**Neural Style-Preserving Visual Dubbing**|Hyeongwoo Kim et.al.|[1909.02518](http://arxiv.org/abs/1909.02518)|null|\n", "1908.03904": "|**2019-08-11**|**Emotion Dependent Facial Animation from Affective Speech**|Rizwan Sadiq et.al.|[1908.03904](http://arxiv.org/abs/1908.03904)|null|\n", "1907.10402": "|**2019-07-24**|**Data-Driven Physical Face Inversion**|Yeara Kozlov et.al.|[1907.10402](http://arxiv.org/abs/1907.10402)|null|\n", "1907.10163": "|**2019-07-23**|**A system for efficient 3D printed stop-motion face animation**|Rinat Abdrashitov et.al.|[1907.10163](http://arxiv.org/abs/1907.10163)|null|\n", "1905.11142": "|**2019-05-27**|**Audio2Face: Generating Speech/Face Animation from Single Audio with Attention-Based Bidirectional LSTM Networks**|Guanzhong Tian et.al.|[1905.11142](http://arxiv.org/abs/1905.11142)|null|\n", "1905.10742": "|**2019-12-20**|**Disentangling Style and Content in Anime Illustrations**|Sitao Xiang et.al.|[1905.10742](http://arxiv.org/abs/1905.10742)|null|\n", "1905.03079": "|**2019-05-08**|**Capture, Learning, and Synthesis of 3D Speaking Styles**|Daniel Cudeiro et.al.|[1905.03079](http://arxiv.org/abs/1905.03079)|**[link](https://github.com/TimoBolkart/voca)**|\n", "1904.01509": "|**2019-04-02**|**FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation**|Yanfu Yan et.al.|[1904.01509](http://arxiv.org/abs/1904.01509)|null|\n", "1903.05448": "|**2019-03-13**|**Animating an Autonomous 3D Talking Avatar**|Dominik Borer et.al.|[1903.05448](http://arxiv.org/abs/1903.05448)|null|\n", "1807.09251": "|**2018-08-28**|**GANimation: Anatomically-aware Facial Animation from a Single Image**|Albert Pumarola et.al.|[1807.09251](http://arxiv.org/abs/1807.09251)|**[link](https://github.com/albertpumarola/GANimation)**|\n", "1805.11714": "|**2018-05-29**|**Deep Video Portraits**|Hyeongwoo Kim et.al.|[1805.11714](http://arxiv.org/abs/1805.11714)|null|\n", "1805.09488": "|**2018-05-24**|**VisemeNet: Audio-Driven Animator-Centric Speech Animation**|Yang Zhou et.al.|[1805.09488](http://arxiv.org/abs/1805.09488)|null|\n", "1805.07997": "|**2018-05-21**|**Anime Style Space Exploration Using Metric Learning and Generative Adversarial Networks**|Sitao Xiang et.al.|[1805.07997](http://arxiv.org/abs/1805.07997)|null|\n", "1710.00920": "|**2017-12-07**|**End-to-end Learning for 3D Facial Animation from Raw Waveforms of Speech**|Hai X. Pham et.al.|[1710.00920](http://arxiv.org/abs/1710.00920)|null|\n", "1707.09629": "|**2017-07-30**|**Kernel Projection of Latent Structures Regression for Facial Animation Retargeting**|Christos Ouzounis et.al.|[1707.09629](http://arxiv.org/abs/1707.09629)|null|\n", "1707.08289": "|**2017-07-26**|**Fast Deep Matting for Portrait Animation on Mobile Phone**|Bingke Zhu et.al.|[1707.08289](http://arxiv.org/abs/1707.08289)|null|\n", "1601.06684": "|**2016-07-11**|**Large-Scale MIMO is Capable of Eliminating Power-Thirsty Channel Coding for Wireless Transmission of HEVC/H.265 Video**|Shaoshi Yang et.al.|[1601.06684](http://arxiv.org/abs/1601.06684)|null|\n", "1512.08212": "|**2016-05-22**|**Improving Facial Analysis and Performance Driven Animation through Disentangling Identity and Expression**|David Rim et.al.|[1512.08212](http://arxiv.org/abs/1512.08212)|null|\n", "1511.06502": "|**2015-11-20**|**ExpressionBot: An Emotive Lifelike Robotic Face for Face-to-Face Communication**|Ali Mollahosseini et.al.|[1511.06502](http://arxiv.org/abs/1511.06502)|null|\n", "1209.4982": "|**2012-09-22**|**Using multimodal speech production data to evaluate articulatory animation for audiovisual speech synthesis**|Ingmar Steiner et.al.|[1209.4982](http://arxiv.org/abs/1209.4982)|null|\n", "1203.6722": "|**2012-03-30**|**Face Expression Recognition and Analysis: The State of the Art**|Vinay Bettadapura et.al.|[1203.6722](http://arxiv.org/abs/1203.6722)|null|\n", "1003.0431": "|**2010-03-01**|**Re-verification of a Lip Synchronization Protocol using Robust Reachability**|Piotr Kordy et.al.|[1003.0431](http://arxiv.org/abs/1003.0431)|null|\n", "0912.0600": "|**2009-12-03**|**Sequential Clustering based Facial Feature Extraction Method for Automatic Creation of Facial Models from Orthogonal Views**|Alireza Ghahari et.al.|[0912.0600](http://arxiv.org/abs/0912.0600)|null|\n", "0812.2988": "|**2008-12-16**|**The Korrontea Data Modeling**|Emmanuel Bouix et.al.|[0812.2988](http://arxiv.org/abs/0812.2988)|null|\n", "cmp-lg/9406002": "|**1994-06-01**|**Speech Dialogue with Facial Displays: Multimodal Human-Computer Conversation**|Katashi Nagao et.al.|[cmp-lg/9406002](http://arxiv.org/abs/cmp-lg/9406002)|null|\n", "2404.12888": "|**2024-04-19**|**Learn2Talk: 3D Talking Face Learns from 2D Talking Face**|Yixiang Zhuang et.al.|[2404.12888](http://arxiv.org/abs/2404.12888)|null|\n", "2404.14037": "|**2024-04-28**|**GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian Splatting**|Hongyun Yu et.al.|[2404.14037](http://arxiv.org/abs/2404.14037)|null|\n", "2404.15264": "|**2024-04-23**|**TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting**|Jiahe Li et.al.|[2404.15264](http://arxiv.org/abs/2404.15264)|null|\n", "2404.16012": "|**2024-04-25**|**GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with Audio-Driven 3D Gaussian Splatting**|Kyusun Cho et.al.|[2404.16012](http://arxiv.org/abs/2404.16012)|**[link](https://github.com/ku-cvlab/gaussiantalker)**|\n", "2404.18604": "|**2024-04-29**|**CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial Animation Generation**|Xiangyu Liang et.al.|[2404.18604](http://arxiv.org/abs/2404.18604)|null|\n", "2404.18501": "|**2024-05-08**|**Audio-Visual Target Speaker Extraction with Reverse Selective Auditory Attention**|Ruijie Tao et.al.|[2404.18501](http://arxiv.org/abs/2404.18501)|null|\n", "2404.19110": "|**2024-04-29**|**EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars**|Nikita Drobyshev et.al.|[2404.19110](http://arxiv.org/abs/2404.19110)|null|\n", "2404.19040": "|**2024-04-29**|**GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable Gaussian Splatting**|Bo Chen et.al.|[2404.19040](http://arxiv.org/abs/2404.19040)|null|\n", "2404.19038": "|**2024-04-29**|**Embedded Representation Learning Network for Animating Styled Video Portrait**|Tianyong Wang et.al.|[2404.19038](http://arxiv.org/abs/2404.19038)|null|\n", "2405.03121": "|**2024-05-06**|**AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding**|Tao Liu et.al.|[2405.03121](http://arxiv.org/abs/2405.03121)|**[link](https://github.com/x-lance/anitalker)**|\n", "2405.04327": "|**2024-05-07**|**Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation**|Dogucan Yaman et.al.|[2405.04327](http://arxiv.org/abs/2405.04327)|null|\n", "2405.05749": "|**2024-05-10**|**NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via Generative Prior**|Gihoon Kim et.al.|[2405.05749](http://arxiv.org/abs/2405.05749)|null|\n", "2405.05636": "|**2024-05-09**|**SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space**|Zeren Zhang et.al.|[2405.05636](http://arxiv.org/abs/2405.05636)|null|\n", "2405.07257": "|**2024-05-12**|**Listen, Disentangle, and Control: Controllable Speech-Driven Talking Head Generation**|Changpeng Cai et.al.|[2405.07257](http://arxiv.org/abs/2405.07257)|null|\n", "2405.08838": "|**2024-05-14**|**PolyGlotFake: A Novel Multilingual and Multimodal DeepFake Dataset**|Yang Hou et.al.|[2405.08838](http://arxiv.org/abs/2405.08838)|**[link](https://github.com/tobuta/PolyGlotFake)**|\n", "2405.10272": "|**2024-05-16**|**Faces that Speak: Jointly Synthesising Talking Face and Speech from Text**|Youngjoon Jang et.al.|[2405.10272](http://arxiv.org/abs/2405.10272)|null|\n", "2405.12970": "|**2024-05-21**|**Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control**|Yue Han et.al.|[2405.12970](http://arxiv.org/abs/2405.12970)|null|\n", "2405.14709": "|**2024-05-28**|**OpFlowTalker: Realistic and Natural Talking Face Generation via Optical Flow Guidance**|Shuheng Ge et.al.|[2405.14709](http://arxiv.org/abs/2405.14709)|null|\n", "2405.13701": "|**2024-05-22**|**Metabook: An Automatically Generated Augmented Reality Storybook Interaction System to Improve Children's Engagement in Storytelling**|Yibo Wang et.al.|[2405.13701](http://arxiv.org/abs/2405.13701)|null|\n", "2405.15758": "|**2024-05-24**|**InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation**|Yuchi Wang et.al.|[2405.15758](http://arxiv.org/abs/2405.15758)|**[link](https://github.com/wangyuchi369/InstructAvatar)**|\n", "2405.19688": "|**2024-06-14**|**DNPM: A Neural Parametric Model for the Synthesis of Facial Geometric Details**|Haitao Cao et.al.|[2405.19688](http://arxiv.org/abs/2405.19688)|null|\n", "2405.21004": "|**2024-05-31**|**MunchSonic: Tracking Fine-grained Dietary Actions through Active Acoustic Sensing on Eyeglasses**|Saif Mahmud et.al.|[2405.21004](http://arxiv.org/abs/2405.21004)|null|\n", "2405.20851": "|**2024-05-31**|**MegActor: Harness the Power of Raw Video for Vivid Portrait Animation**|Shurong Yang et.al.|[2405.20851](http://arxiv.org/abs/2405.20851)|**[link](https://github.com/megvii-research/megfaceanimate)**|\n", "2405.20412": "|**2024-05-30**|**Audio2Rig: Artist-oriented deep learning tool for facial animation**|Bastien Arcelin et.al.|[2405.20412](http://arxiv.org/abs/2405.20412)|null|\n", "2406.01900": "|**2024-06-07**|**Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation**|Yue Ma et.al.|[2406.01900](http://arxiv.org/abs/2406.01900)|null|\n", "2406.02880": "|**2024-06-05**|**Controllable Talking Face Generation by Implicit Facial Keypoints Editing**|Dong Zhao et.al.|[2406.02880](http://arxiv.org/abs/2406.02880)|null|\n", "2406.08096": "|**2024-06-17**|**Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with Motion and Appearance Disentanglement**|Runyi Yu et.al.|[2406.08096](http://arxiv.org/abs/2406.08096)|null|\n", "2406.07895": "|**2024-06-12**|**Emotional Conversation: Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation**|Jiadong Liang et.al.|[2406.07895](http://arxiv.org/abs/2406.07895)|null|\n", "2406.08802": "|**2024-06-13**|**DubWise: Video-Guided Speech Duration Control in Multimodal LLM-based Text-to-Speech for Dubbing**|Neha Sahipjohn et.al.|[2406.08802](http://arxiv.org/abs/2406.08802)|null|\n", "2406.08801": "|**2024-06-16**|**Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation**|Mingwang Xu et.al.|[2406.08801](http://arxiv.org/abs/2406.08801)|null|\n", "2406.09519": "|**2024-06-13**|**Talking Heads: Understanding Inter-layer Communication in Transformer Language Models**|Jack Merullo et.al.|[2406.09519](http://arxiv.org/abs/2406.09519)|null|\n", "2406.11597": "|**2024-06-17**|**Compressed Skinning for Facial Blendshapes**|Ladislav Kavan et.al.|[2406.11597](http://arxiv.org/abs/2406.11597)|null|\n", "2406.11259": "|**2024-06-17**|**NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation**|Niu Guanchen et.al.|[2406.11259](http://arxiv.org/abs/2406.11259)|null|\n", "2406.10553": "|**2024-06-18**|**A Comprehensive Taxonomy and Analysis of Talking Head Synthesis: Techniques for Portrait Generation, Driving Mechanisms, and Editing**|Ming Meng et.al.|[2406.10553](http://arxiv.org/abs/2406.10553)|null|\n", "2406.14272": "|**2024-06-20**|**MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset**|Kim Sung-Bin et.al.|[2406.14272](http://arxiv.org/abs/2406.14272)|null|\n", "2406.13495": "|**2024-06-19**|**DF40: Toward Next-Generation Deepfake Detection**|Zhiyuan Yan et.al.|[2406.13495](http://arxiv.org/abs/2406.13495)|null|\n", "2406.13272": "|**2024-06-19**|**AniFaceDiff: High-Fidelity Face Reenactment via Facial Parametric Conditioned Diffusion Models**|Ken Chen et.al.|[2406.13272](http://arxiv.org/abs/2406.13272)|null|\n", "2406.13093": "|**2024-06-18**|**RITA: A Real-time Interactive Talking Avatars Framework**|Wuxinlin Cheng et.al.|[2406.13093](http://arxiv.org/abs/2406.13093)|null|\n", "2406.15177": "|**2024-06-21**|**EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot**|Hao Fei et.al.|[2406.15177](http://arxiv.org/abs/2406.15177)|**[link](https://github.com/scofield7419/empathyear)**|\n", "2406.18284": "|**2024-06-26**|**RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D Facial Prior-guided Identity Alignment Network**|Xiaozhong Ji et.al.|[2406.18284](http://arxiv.org/abs/2406.18284)|null|\n", "2407.01034": "|**2024-07-01**|**Enhancing Speech-Driven 3D Facial Animation with Audio-Visual Guidance from Lip Reading Expert**|Han EunGi et.al.|[2407.01034](http://arxiv.org/abs/2407.01034)|null|\n", "2407.03168": "|**2024-07-03**|**LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control**|Jianzhu Guo et.al.|[2407.03168](http://arxiv.org/abs/2407.03168)|null|\n"}, "Image Animation": {"2401.01827": "|**2024-01-03**|**Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions**|David Junhao Zhang et.al.|[2401.01827](http://arxiv.org/abs/2401.01827)|**[link](https://github.com/salesforce/lavis)**|\n", "2312.13964": "|**2024-03-25**|**PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models**|Yiming Zhang et.al.|[2312.13964](http://arxiv.org/abs/2312.13964)|**[link](https://github.com/open-mmlab/PIA)**|\n", "2312.03793": "|**2023-12-06**|**AnimateZero: Video Diffusion Models are Zero-Shot Image Animators**|Jiwen Yu et.al.|[2312.03793](http://arxiv.org/abs/2312.03793)|**[link](https://github.com/vvictoryuki/animatezero)**|\n", "2312.02928": "|**2023-12-05**|**LivePhoto: Real Image Animation with Text-guided Motion Control**|Xi Chen et.al.|[2312.02928](http://arxiv.org/abs/2312.02928)|null|\n", "2311.18827": "|**2023-11-30**|**Motion-Conditioned Image Animation for Video Editing**|Wilson Yan et.al.|[2311.18827](http://arxiv.org/abs/2311.18827)|null|\n", "2311.16498": "|**2023-11-27**|**MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model**|Zhongcong Xu et.al.|[2311.16498](http://arxiv.org/abs/2311.16498)|null|\n", "2311.12886": "|**2023-12-04**|**AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance**|Zuozhuo Dai et.al.|[2311.12886](http://arxiv.org/abs/2311.12886)|**[link](https://github.com/alibaba/animate-anything)**|\n", "2310.12190": "|**2023-11-27**|**DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors**|Jinbo Xing et.al.|[2310.12190](http://arxiv.org/abs/2310.12190)|**[link](https://github.com/Doubiiu/DynamiCrafter)**|\n", "2310.10769": "|**2023-10-16**|**LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation**|Ruiqi Wu et.al.|[2310.10769](http://arxiv.org/abs/2310.10769)|**[link](https://github.com/RQ-Wu/LAMP)**|\n", "2309.14207": "|**2023-09-25**|**Automatic Animation of Hair Blowing in Still Portrait Photos**|Wenpeng Xiao et.al.|[2309.14207](http://arxiv.org/abs/2309.14207)|null|\n", "2307.04725": "|**2023-07-10**|**AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning**|Yuwei Guo et.al.|[2307.04725](http://arxiv.org/abs/2307.04725)|**[link](https://github.com/guoyww/animatediff)**|\n", "2307.04187": "|**2023-07-09**|**Predictive Coding For Animation-Based Video Compression**|Goluck Konuko et.al.|[2307.04187](http://arxiv.org/abs/2307.04187)|null|\n", "2307.03190": "|**2023-09-26**|**Text-Guided Synthesis of Eulerian Cinemagraphs**|Aniruddha Mahapatra et.al.|[2307.03190](http://arxiv.org/abs/2307.03190)|**[link](https://github.com/text2cinemagraph/text2cinemagraph)**|\n", "2305.03989": "|**2023-10-11**|**LEO: Generative Latent Image Animator for Human Video Synthesis**|Yaohui Wang et.al.|[2305.03989](http://arxiv.org/abs/2305.03989)|**[link](https://github.com/wyhsirius/LEO)**|\n", "2304.06020": "|**2023-04-12**|**VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs**|Moayed Haji Ali et.al.|[2304.06020](http://arxiv.org/abs/2304.06020)|null|\n", "2303.05724": "|**2023-03-10**|**3D Cinemagraphy from a Single Image**|Xingyi Li et.al.|[2303.05724](http://arxiv.org/abs/2303.05724)|null|\n", "2302.01329": "|**2023-02-02**|**Dreamix: Video Diffusion Models are General Video Editors**|Eyal Molad et.al.|[2302.01329](http://arxiv.org/abs/2302.01329)|null|\n", "2301.05905": "|**2023-01-14**|**Continuous odor profile monitoring to study olfactory navigation in small animals**|Kevin S. Chen et.al.|[2301.05905](http://arxiv.org/abs/2301.05905)|null|\n", "2211.17235": "|**2022-11-30**|**NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation**|Yu Yin et.al.|[2211.17235](http://arxiv.org/abs/2211.17235)|null|\n", "2210.01794": "|**2022-10-04**|**Implicit Warping for Animation with Image Sets**|Arun Mallya et.al.|[2210.01794](http://arxiv.org/abs/2210.01794)|null|\n", "2209.14024": "|**2022-09-28**|**Motion Transformer for Unsupervised Image Animation**|Jiale Tao et.al.|[2209.14024](http://arxiv.org/abs/2209.14024)|**[link](https://github.com/jialetao/motrans)**|\n", "2207.09161": "|**2022-07-19**|**Single Stage Virtual Try-on via Deformable Attention Flows**|Shuai Bai et.al.|[2207.09161](http://arxiv.org/abs/2207.09161)|**[link](https://github.com/OFA-Sys/DAFlow)**|\n", "2207.03714": "|**2022-07-08**|**Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation**|Yucheng Suo et.al.|[2207.03714](http://arxiv.org/abs/2207.03714)|null|\n", "2203.14367": "|**2022-03-29**|**Thin-Plate Spline Motion Model for Image Animation**|Jian Zhao et.al.|[2203.14367](http://arxiv.org/abs/2203.14367)|**[link](https://github.com/yoyo-nb/thin-plate-spline-motion-model)**|\n", "2203.13441": "|**2022-03-25**|**3D GAN Inversion for Controllable Portrait Image Animation**|Connor Z. Lin et.al.|[2203.13441](http://arxiv.org/abs/2203.13441)|null|\n", "2203.09043": "|**2022-03-17**|**Latent Image Animator: Learning to Animate Images via Latent Space Navigation**|Yaohui Wang et.al.|[2203.09043](http://arxiv.org/abs/2203.09043)|null|\n", "2112.10457": "|**2021-12-21**|**Image Animation with Keypoint Mask**|Or Toledano et.al.|[2112.10457](http://arxiv.org/abs/2112.10457)|**[link](https://github.com/or-toledano/animation-with-keypoint-mask)**|\n", "2112.13647": "|**2021-12-19**|**Move As You Like: Image Animation in E-Commerce Scenario**|Borun Xu et.al.|[2112.13647](http://arxiv.org/abs/2112.13647)|null|\n", "2112.09401": "|**2021-12-17**|**AI-Empowered Persuasive Video Generation: A Survey**|Chang Liu et.al.|[2112.09401](http://arxiv.org/abs/2112.09401)|null|\n", "2111.11426": "|**2022-04-05**|**Neural Fields in Visual Computing and Beyond**|Yiheng Xie et.al.|[2111.11426](http://arxiv.org/abs/2111.11426)|null|\n", "2110.13598": "|**2021-10-26**|**Incremental Learning for Animal Pose Estimation using RBF k-DPP**|Gaurav Kumar Nayak et.al.|[2110.13598](http://arxiv.org/abs/2110.13598)|null|\n", "2110.04658": "|**2023-11-19**|**Differential Motion Evolution for Fine-Grained Motion Deformation in Unsupervised Image Animation**|Peirong Liu et.al.|[2110.04658](http://arxiv.org/abs/2110.04658)|null|\n", "2109.00471": "|**2021-09-03**|**Sparse to Dense Motion Transfer for Face Image Animation**|Ruiqi Zhao et.al.|[2109.00471](http://arxiv.org/abs/2109.00471)|null|\n", "2108.07949": "|**2021-08-18**|**DeepFake MNIST+: A DeepFake Facial Animation Dataset**|Jiajun Huang et.al.|[2108.07949](http://arxiv.org/abs/2108.07949)|**[link](https://github.com/huangjiadidi/DeepFakeMnist)**|\n", "2106.15342": "|**2021-06-23**|**Analisis Kualitas Layanan Website E-Commerce Bukalapak Terhadap Kepuasan Pengguna Mahasiswa Universitas Bina Darma Menggunakan Metode Webqual 4.0**|Adellia et.al.|[2106.15342](http://arxiv.org/abs/2106.15342)|null|\n", "2106.12284": "|**2022-06-11**|**Bayesian Statistics Guided Label Refurbishment Mechanism: Mitigating Label Noise in Medical Image Classification**|Mengdi Gao et.al.|[2106.12284](http://arxiv.org/abs/2106.12284)|**[link](https://github.com/neugmd/blrm)**|\n", "2104.03117": "|**2021-04-07**|**Single Source One Shot Reenactment using Weighted motion From Paired Feature Points**|Soumya Tripathy et.al.|[2104.03117](http://arxiv.org/abs/2104.03117)|null|\n", "2103.11600": "|**2021-03-22**|**PriorityCut: Occlusion-guided Regularization for Warp-based Image Animation**|Wai Ting Cheung et.al.|[2103.11600](http://arxiv.org/abs/2103.11600)|null|\n", "2012.00346": "|**2020-12-01**|**Ultra-low bitrate video conferencing using deep image animation**|Goluck Konuko et.al.|[2012.00346](http://arxiv.org/abs/2012.00346)|null|\n", "2011.06922": "|**2022-03-29**|**Image Animation with Perturbed Masks**|Yoav Shalev et.al.|[2011.06922](http://arxiv.org/abs/2011.06922)|**[link](https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks)**|\n", "2008.12606": "|**2020-08-27**|**Deep Spatial Transformation for Pose-Guided Person Image Generation and Animation**|Yurui Ren et.al.|[2008.12606](http://arxiv.org/abs/2008.12606)|**[link](https://github.com/RenYurui/Global-Flow-Local-Attention)**|\n", "2003.00196": "|**2020-10-01**|**First Order Motion Model for Image Animation**|Aliaksandr Siarohin et.al.|[2003.00196](http://arxiv.org/abs/2003.00196)|**[link](https://github.com/AliaksandrSiarohin/first-order-model)**|\n", "1812.08861": "|**2019-08-30**|**Animating Arbitrary Objects via Deep Motion Transfer**|Aliaksandr Siarohin et.al.|[1812.08861](http://arxiv.org/abs/1812.08861)|**[link](https://github.com/AliaksandrSiarohin/monkey-net)**|\n", "1810.03956": "|**2018-10-09**|**3D model silhouette-based tracking in depth images for puppet suit dynamic video-mapping**|Guillaume Caron et.al.|[1810.03956](http://arxiv.org/abs/1810.03956)|null|\n", "1806.09117": "|**2018-06-24**|**A Design of FPGA Based Small Animal PET Real Time Digital Signal Processing and Correction Logic**|Jiaming Lu et.al.|[1806.09117](http://arxiv.org/abs/1806.09117)|null|\n", "1801.10452": "|**2018-01-31**|**RAPTOR I: Time-dependent radiative transfer in arbitrary spacetimes**|Thomas Bronzwaer et.al.|[1801.10452](http://arxiv.org/abs/1801.10452)|null|\n", "1606.07189": "|**2016-06-23**|**Gender and Interest Targeting for Sponsored Post Advertising at Tumblr**|Mihajlo Grbovic et.al.|[1606.07189](http://arxiv.org/abs/1606.07189)|null|\n", "1503.04837": "|**2015-03-16**|**Use of Effective Audio in E-learning Courseware**|Kisor Ray et.al.|[1503.04837](http://arxiv.org/abs/1503.04837)|null|\n", "1502.01090": "|**2015-02-04**|**Multimedia-Video for Learning**|Kah Hean Chua et.al.|[1502.01090](http://arxiv.org/abs/1502.01090)|null|\n", "1301.6130": "|**2013-01-25**|**Measurements of Martian Dust Devil Winds with HiRISE**|David S. Choi et.al.|[1301.6130](http://arxiv.org/abs/1301.6130)|null|\n", "1001.0440": "|**2010-01-04**|**Tutoring System for Dance Learning**|Rajkumar Kannan et.al.|[1001.0440](http://arxiv.org/abs/1001.0440)|null|\n", "2401.09146": "|**2024-01-17**|**Continuous Piecewise-Affine Based Motion Model for Image Animation**|Hexiang Wang et.al.|[2401.09146](http://arxiv.org/abs/2401.09146)|**[link](https://github.com/devilpg/aaai2024-cpabmm)**|\n", "2403.02827": "|**2024-03-05**|**Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation**|Weijie Li et.al.|[2403.02827](http://arxiv.org/abs/2403.02827)|null|\n", "2403.05659": "|**2024-03-08**|**Audio-Synchronized Visual Animation**|Lin Zhang et.al.|[2403.05659](http://arxiv.org/abs/2403.05659)|null|\n", "2403.08268": "|**2024-03-13**|**Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts**|Yue Ma et.al.|[2403.08268](http://arxiv.org/abs/2403.08268)|**[link](https://github.com/mayuelala/followyourclick)**|\n", "2403.14781": "|**2024-06-01**|**Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance**|Shenhao Zhu et.al.|[2403.14781](http://arxiv.org/abs/2403.14781)|**[link](https://github.com/fudan-generative-vision/champ)**|\n", "2405.17306": "|**2024-05-28**|**Controllable Longer Image Animation with Diffusion Models**|Qiang Wang et.al.|[2405.17306](http://arxiv.org/abs/2405.17306)|null|\n", "2405.18156": "|**2024-05-28**|**VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation**|Qilin Wang et.al.|[2405.18156](http://arxiv.org/abs/2405.18156)|null|\n", "2405.18908": "|**2024-05-29**|**Evaluating the efectiveness of sonifcation in science education using Edukoi**|Lucrezia Guiotto Nai Fovino et.al.|[2405.18908](http://arxiv.org/abs/2405.18908)|null|\n", "2405.20222": "|**2024-06-02**|**MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model**|Muyao Niu et.al.|[2405.20222](http://arxiv.org/abs/2405.20222)|**[link](https://github.com/myniuuu/mofa-video)**|\n", "2406.01188": "|**2024-06-03**|**UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation**|Xiang Wang et.al.|[2406.01188](http://arxiv.org/abs/2406.01188)|null|\n", "2406.03035": "|**2024-06-13**|**Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control**|Jingyun Xue et.al.|[2406.03035](http://arxiv.org/abs/2406.03035)|null|\n", "2406.08801": "|**2024-06-16**|**Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation**|Mingwang Xu et.al.|[2406.08801](http://arxiv.org/abs/2406.08801)|null|\n"}}